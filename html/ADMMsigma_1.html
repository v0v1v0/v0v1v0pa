<div class="container">

<table style="width: 100%;"><tr>
<td>ADMMc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Penalized precision matrix estimation via ADMM (c++)</h2>

<h3>Description</h3>

<p>Penalized precision matrix estimation using the ADMM algorithm
</p>


<h3>Usage</h3>

<pre><code class="language-R">ADMMc(S, initOmega, initZ, initY, lam, alpha = 1, diagonal = FALSE,
  rho = 2, mu = 10, tau_inc = 2, tau_dec = 2, crit = "ADMM",
  tol_abs = 1e-04, tol_rel = 1e-04, maxit = 10000L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>pxp sample covariance matrix (denominator n).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initOmega</code></td>
<td>
<p>initialization matrix for Omega</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initZ</code></td>
<td>
<p>initialization matrix for Z</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initY</code></td>
<td>
<p>initialization matrix for Y</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam</code></td>
<td>
<p>postive tuning parameter for elastic net penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>elastic net mixing parameter contained in [0, 1]. <code>0 = ridge, 1 = lasso</code>. Defaults to alpha = 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diagonal</code></td>
<td>
<p>option to penalize the diagonal elements of the estimated precision matrix (<code class="reqn">\Omega</code>). Defaults to <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>initial step size for ADMM algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>factor for primal and residual norms in the ADMM algorithm. This will be used to adjust the step size <code>rho</code> after each iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau_inc</code></td>
<td>
<p>factor in which to increase step size <code>rho</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau_dec</code></td>
<td>
<p>factor in which to decrease step size <code>rho</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>crit</code></td>
<td>
<p>criterion for convergence (<code>ADMM</code> or <code>loglik</code>). If <code>crit = loglik</code> then iterations will stop when the relative change in log-likelihood is less than <code>tol.abs</code>. Default is <code>ADMM</code> and follows the procedure outlined in Boyd, et al.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol_abs</code></td>
<td>
<p>absolute convergence tolerance. Defaults to 1e-4.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol_rel</code></td>
<td>
<p>relative convergence tolerance. Defaults to 1e-4.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>maximum number of iterations. Defaults to 1e4.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For details on the implementation of 'ADMMsigma', see the vignette
<a href="https://mgallow.github.io/ADMMsigma/">https://mgallow.github.io/ADMMsigma/</a>.
</p>


<h3>Value</h3>

<p>returns list of returns which includes:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Iterations</code></td>
<td>
<p>number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam</code></td>
<td>
<p>optimal tuning parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>optimal tuning parameter.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Omega</code></td>
<td>
<p>estimated penalized precision matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Z2</code></td>
<td>
<p>estimated Z matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>estimated Y matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>estimated rho.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Matt Galloway <a href="mailto:gall0441@umn.edu">gall0441@umn.edu</a>
</p>


<h3>References</h3>


<ul>
<li>
<p> Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. 'Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.' <em>Foundations and Trends in Machine Learning</em> 3 (1). Now Publishers, Inc.: 1-122. <a href="https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf</a>
</p>
</li>
<li>
<p> Hu, Yue, Chi, Eric C, amd Allen, Genevera I. 2016. 'ADMM Algorithmic Regularization Paths for Sparse Statistical Machine Learning.' <em>Splitting Methods in Communication, Imaging, Science, and Engineering</em>. Springer: 433-459.
</p>
</li>
<li>
<p> Zou, Hui and Hastie, Trevor. 2005. "Regularization and Variable Selection via the Elastic Net." <em>Journal of the Royal Statistial Society: Series B (Statistical Methodology)</em> 67 (2). Wiley Online Library: 301-320.
</p>
</li>
<li>
<p> Rothman, Adam. 2017. 'STAT 8931 notes on an algorithm to compute the Lasso-penalized Gaussian likelihood precision matrix estimator.'
</p>
</li>
</ul>
</div>