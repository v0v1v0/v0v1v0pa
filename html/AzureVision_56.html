<div class="container">

<table style="width: 100%;"><tr>
<td>analyze</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Interface to Azure Computer Vision API</h2>

<h3>Description</h3>

<p>Interface to Azure Computer Vision API
</p>


<h3>Usage</h3>

<pre><code class="language-R">analyze(endpoint, image, domain = NULL, feature_types = NULL,
  language = "en", ...)

describe(endpoint, image, language = "en", ...)

detect_objects(endpoint, image, ...)

area_of_interest(endpoint, image, ...)

tag(endpoint, image, language = "en", ...)

categorize(endpoint, image, ...)

read_text(endpoint, image, detect_orientation = TRUE, language = "en", ...)

list_computervision_domains(endpoint, ...)

make_thumbnail(endpoint, image, outfile, width = 50, height = 50,
  smart_crop = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>endpoint</code></td>
<td>
<p>A computer vision endpoint.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>image</code></td>
<td>
<p>An image to be sent to the endpoint. This can be either a filename, a publicly accessible URL, or a raw vector holding the file contents.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>domain</code></td>
<td>
<p>For <code>analyze</code>, an optional domain-specific model to use to analyze the image. Can be "celebrities" or "landmarks".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>feature_types</code></td>
<td>
<p>For <code>analyze</code>, an optional character vector of more detailed features to return. This can be one or more of: "categories", "tags", "description", "faces", "imagetype", "color", "adult", "brands" and "objects". If not supplied, defaults to "categories".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>language</code></td>
<td>
<p>A 2-character code indicating the language to use for tags, feature labels and descriptions. The default is <code>en</code>, for English.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Arguments passed to lower-level functions, and ultimately to <code>call_cognitive_endpoint</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>detect_orientation</code></td>
<td>
<p>For <code>read_text</code>, whether to automatically determine the image's orientation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outfile</code></td>
<td>
<p>For <code>make_thumbnail</code>, the filename for the generated thumbnail. Alternatively, if this is NULL the thumbnail is returned as a raw vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>width, height</code></td>
<td>
<p>For <code>make_thumbnail</code>, the dimensions for the returned thumbnail.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>smart_crop</code></td>
<td>
<p>For <code>make_thumbnail</code>, whether to automatically determine the best location to crop for the thumbnail. Useful when the aspect ratios of the original image and the thumbnail don't match.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>analyze</code> extracts visual features from the image. To obtain more detailed features, specify the <code>domain</code> and/or <code>feature_types</code> arguments as appropriate.
</p>
<p><code>describe</code> attempts to provide a text description of the image.
</p>
<p><code>detect_objects</code> detects objects in the image.
</p>
<p><code>area_of_interest</code> attempts to find the "interesting" part of an image, meaning the most likely location of the image's subject.
</p>
<p><code>tag</code> returns a set of words that are relevant to the content of the image. Not to be confused with the <code>add_tags</code> or <code>add_image_tags</code> functions that are part of the Custom Vision API.
</p>
<p><code>categorize</code> attempts to place the image into a list of predefined categories.
</p>
<p><code>read_text</code> performs optical character recognition (OCR) on the image.
</p>
<p><code>list_domains</code> returns the predefined domain-specific models that can be queried by <code>analyze</code> for deeper analysis. Not to be confused with the domains available for training models with the Custom Vision API.
</p>
<p><code>make_thumbnail</code> generates a thumbnail of the image, with the specified dimensions.
</p>


<h3>Value</h3>

<p><code>analyze</code> returns a list containing the results of the analysis. The components will vary depending on the domain and feature types requested.
</p>
<p><code>describe</code> returns a list with two components: <code>tags</code>, a vector of text labels; and <code>captions</code>, a data frame of descriptive sentences.
</p>
<p><code>detect_objects</code>  returns a dataframe giving the locations and types of the detected objects.
</p>
<p><code>area_of_interest</code> returns a length-4 numeric vector, containing the top-left coordinates of the area of interest and its width and height.
</p>
<p><code>tag</code> and <code>categorize</code> return a data frame of tag and category information, respectively.
</p>
<p><code>read_text</code> returns the extracted text as a list with one component per region that contains text. Each component is a vector of character strings.
</p>
<p><code>list_computervision_domains</code> returns a character vector of domain names.
</p>
<p><code>make_thumbnail</code> returns a raw vector holding the contents of the thumbnail, if the <code>outfile</code> argument is NULL. Otherwise, the thumbnail is saved into <code>outfile</code>.
</p>


<h3>See Also</h3>

<p><code>computervision_endpoint</code>, <code>AzureCognitive::call_cognitive_endpoint</code>
</p>
<p><a href="https://docs.microsoft.com/en-us/azure/cognitive-services/Computer-vision/Home">Computer Vision documentation</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 

vis &lt;- computervision_endpoint(
    url="https://accountname.cognitiveservices.azure.com/",
    key="account_key"
)

list_domains(vis)

# analyze a local file
analyze(vis, "image.jpg")
# picture on the Internet
analyze(vis, "https://example.com/image.jpg")
# as a raw vector
analyze(vis, readBin("image.jpg", "raw", file.size("image.jpg")))

# analyze has optional extras
analyze(vis, "image.jpg", feature_types=c("faces", "objects"))

describe(vis, "image.jpg")
detect_objects(vis, "image.jpg")
area_of_interest(vis, "image.jpg")
tag(vis, "image.jpg")  # more reliable than analyze(*, feature_types="tags")
categorize(vis, "image.jpg")
read_text(vis, "scanned_text.jpg")


## End(Not run)
</code></pre>


</div>