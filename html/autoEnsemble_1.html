<div class="container">

<table style="width: 100%;"><tr>
<td>ensemble</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Evaluate H2O Model(s) Performance</h2>

<h3>Description</h3>

<p>Multiple model performance metrics are computed
</p>


<h3>Usage</h3>

<pre><code class="language-R">ensemble(
  models,
  training_frame,
  newdata = NULL,
  family = "binary",
  strategy = c("search"),
  model_selection_criteria = c("auc", "aucpr", "mcc", "f2"),
  min_improvement = 1e-05,
  max = NULL,
  top_rank = seq(0.01, 0.99, 0.01),
  stop_rounds = 3,
  reset_stop_rounds = TRUE,
  stop_metric = "auc",
  seed = -1,
  verbatim = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>models</code></td>
<td>
<p>H2O search grid or AutoML grid or a character vector of H2O model IDs.
the <code>"h2o.get_ids"</code> function from <code>"h2otools"</code> can
retrieve the IDs from grids.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>training_frame</code></td>
<td>
<p>h2o training frame (data.frame) for model training</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>
<p>h2o frame (data.frame). the data.frame must be already uploaded
on h2o server (cloud). when specified, this dataset will be used
for evaluating the models. if not specified, model performance
on the training dataset will be reported.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>model family. currently only <code>"binary"</code> classification models
are supported.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>strategy</code></td>
<td>
<p>character. the current available strategies are <code>"search"</code>
(default) and <code>"top"</code>. The <code>"search"</code> strategy searches
for the best combination of top-performing diverse models
whereas the <code>"top"</code> strategy is more simplified and just
combines the specified of top-performing diverse models without
examining the possibility of improving the model by searching for
larger number of models that can further improve the model. generally,
the <code>"search"</code> strategy is preferable, unless the computation
runtime is too large and optimization is not possible.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model_selection_criteria</code></td>
<td>
<p>character, specifying the performance metrics that
should be taken into consideration for model selection. the default are
<code>"c('auc', 'aucpr', 'mcc', 'f2')"</code>. other possible criteria are
<code>"'f1point5', 'f3', 'f4', 'f5', 'kappa', 'mean_per_class_error', 'gini', 'accuracy'"</code>,
which are also provided by the <code>"evaluate"</code> function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_improvement</code></td>
<td>
<p>numeric. specifies the minimum improvement in model
evaluation metric to qualify further optimization search.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max</code></td>
<td>
<p>integer. specifies maximum number of models for each criteria to be extracted. the
default value is the <code>"top_rank"</code> percentage for each model selection
criteria.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>top_rank</code></td>
<td>
<p>numeric vector. specifies percentage of the top models taht
should be selected. if the strategy is <code>"search"</code>, the
algorithm searches for the best best combination of the models
from top ranked models to the bottom. however, if the strategy
is <code>"top"</code>, only the first value of the vector is used
(default value is top 1%).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop_rounds</code></td>
<td>
<p>integer. number of stoping rounds, in case the model stops
improving</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reset_stop_rounds</code></td>
<td>
<p>logical. if TRUE, everytime the model improves the
stopping rounds penalty is resets to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop_metric</code></td>
<td>
<p>character. model stopping metric. the default is <code>"auc"</code>,
but <code>"aucpr"</code> and <code>"mcc"</code> are also available.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>random seed (recommended)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbatim</code></td>
<td>
<p>logical. if TRUE, it reports additional information about the
progress of the model training, particularly used for debugging.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a matrix of F-Measures for different thresholds or the highest F-Measure value
</p>


<h3>Author(s)</h3>

<p>E. F. Haghish
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Not run: 
# load the required libraries for building the base-learners and the ensemble models
library(h2o)
library(autoEnsemble)

# initiate the h2o server
h2o.init(ignore_config = TRUE, nthreads = 2, bind_to_localhost = FALSE, insecure = TRUE)

# upload data to h2o cloud
prostate_path &lt;- system.file("extdata", "prostate.csv", package = "h2o")
prostate &lt;- h2o.importFile(path = prostate_path, header = TRUE)

### H2O provides 2 types of grid search for tuning the models, which are
### AutoML and Grid. Below, I tune 2 set of model grids and use them both
### for building the ensemble, just to set an example ...

#######################################################
### PREPARE AutoML Grid (takes a couple of minutes)
#######################################################
# run AutoML to tune various models (GLM, GBM, XGBoost, DRF, DeepLearning) for 120 seconds
y &lt;- "CAPSULE"
prostate[,y] &lt;- as.factor(prostate[,y])  #convert to factor for classification
aml &lt;- h2o.automl(y = y, training_frame = prostate, max_runtime_secs = 120,
                 include_algos=c("DRF","GLM", "XGBoost", "GBM", "DeepLearning"),

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

#######################################################
### PREPARE H2O Grid (takes a couple of minutes)
#######################################################
# make sure equal number of "nfolds" is specified for different grids
grid &lt;- h2o.grid(algorithm = "gbm", y = y, training_frame = prostate,
                 hyper_params = list(ntrees = seq(1,50,1)),
                 grid_id = "ensemble_grid",

                 # this setting ensures the models are comparable for building a meta learner
                 seed = 2023, fold_assignment = "Modulo", nfolds = 10,
                 keep_cross_validation_predictions = TRUE)

#######################################################
### PREPARE ENSEMBLE MODEL
#######################################################

### get the models' IDs from the AutoML and grid searches.
### this is all that is needed before building the ensemble,
### i.e., to specify the model IDs that should be evaluated.

ids    &lt;- c(h2o.get_ids(aml), h2o.get_ids(grid))
top    &lt;- ensemble(models = ids, training_frame = prostate, strategy = "top")
search &lt;- ensemble(models = ids, training_frame = prostate, strategy = "search")

#######################################################
### EVALUATE THE MODELS
#######################################################
h2o.auc(aml@leader)                          # best model identified by h2o.automl
h2o.auc(h2o.getModel(grid@model_ids[[1]]))   # best model identified by grid search
h2o.auc(top).                                # ensemble model with 'top' search strategy
h2o.auc(search).                             # ensemble model with 'search' search strategy


## End(Not run)
</code></pre>


</div>