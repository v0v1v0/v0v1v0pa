<div class="container">

<table style="width: 100%;"><tr>
<td>auto_tune_xgboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>auto_tune_xgboost</h2>

<h3>Description</h3>

<p>Automatically tunes an xgboost model using grid or bayesian optimization
</p>


<h3>Usage</h3>

<pre><code class="language-R">auto_tune_xgboost(
  .data,
  formula,
  tune_method = c("grid", "bayes"),
  event_level = c("first", "second"),
  n_fold = 5L,
  n_iter = 100L,
  seed = 1,
  save_output = FALSE,
  parallel = TRUE,
  trees = tune::tune(),
  min_n = tune::tune(),
  mtry = tune::tune(),
  tree_depth = tune::tune(),
  learn_rate = tune::tune(),
  loss_reduction = tune::tune(),
  sample_size = tune::tune(),
  stop_iter = tune::tune(),
  counts = FALSE,
  tree_method = c("auto", "exact", "approx", "hist", "gpu_hist"),
  monotone_constraints = 0L,
  num_parallel_tree = 1L,
  lambda = 1,
  alpha = 0,
  scale_pos_weight = 1,
  verbosity = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>.data</code></td>
<td>
<p>dataframe</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>formula</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tune_method</code></td>
<td>
<p>method of tuning. defaults to grid</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>event_level</code></td>
<td>
<p>for binary classification, which factor level is the positive class. specify "second" for second level</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_fold</code></td>
<td>
<p>integer. n folds in resamples</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_iter</code></td>
<td>
<p>n iterations for tuning (bayes); paramter grid size (grid)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>seed</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save_output</code></td>
<td>
<p>FASLE. If set to TRUE will write the output as an rds file</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>
<p>default TRUE; If set to TRUE, will enable parallel processing on resamples for grid tuning</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trees</code></td>
<td>
<p># Trees (xgboost: nrounds) (type: integer, default: 500L)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_n</code></td>
<td>
<p>Minimal Node Size (xgboost: min_child_weight) (type: integer, default: 2L); [typical range: 2-10] Keep small value for highly imbalanced class data where leaf nodes can have smaller size groups. Otherwise increase size to prevent overfitting outliers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtry</code></td>
<td>
<p># Randomly Selected Predictors; defaults to .75; (xgboost: colsample_bynode) (type: numeric, range 0 - 1) (or type: integer if <code>count = TRUE</code>)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tree_depth</code></td>
<td>
<p>Tree Depth (xgboost: max_depth) (type: integer, default: 7L); Typical values: 3-10</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learn_rate</code></td>
<td>
<p>Learning Rate (xgboost: eta) (type: double, default: 0.05); Typical values: 0.01-0.3</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loss_reduction</code></td>
<td>
<p>Minimum Loss Reduction (xgboost: gamma) (type: double, default: 1.0);  range: 0 to Inf; typical value: 0 - 20 assuming low-mid tree depth</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_size</code></td>
<td>
<p>Proportion Observations Sampled (xgboost: subsample) (type: double, default: .75); Typical values: 0.5 - 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stop_iter</code></td>
<td>
<p># Iterations Before Stopping (xgboost: early_stop) (type: integer, default: 15L) only enabled if validation set is provided</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>counts</code></td>
<td>
<p>if <code>TRUE</code> specify <code>mtry</code> as an integer number of cols. Default <code>FALSE</code> to specify <code>mtry</code> as fraction of cols from 0 to 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tree_method</code></td>
<td>
<p>xgboost tree_method. default is <code>auto</code>. reference: <a href="https://xgboost.readthedocs.io/en/stable/treemethod.html">tree method docs</a></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>monotone_constraints</code></td>
<td>
<p>an integer vector with length of the predictor cols, of <code>-1, 1, 0</code> corresponding to decreasing, increasing, and no constraint respectively for the index of the predictor col. reference: <a href="https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html">monotonicity docs</a>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_parallel_tree</code></td>
<td>
<p>should be set to the size of the forest being trained. default 1L</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>[default=.5] L2 regularization term on weights. Increasing this value will make model more conservative.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>[default=.1] L1 regularization term on weights. Increasing this value will make model more conservative.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale_pos_weight</code></td>
<td>
<p>[default=1] Control the balance of positive and negative weights, useful for unbalanced classes. if set to TRUE, calculates sum(negative instances) / sum(positive instances). If first level is majority class, use values &lt; 1, otherwise normally values &gt;1 are used to balance the class distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbosity</code></td>
<td>
<p>[default=1] Verbosity of printing messages. Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Default is to tune all 7 xgboost parameters. Individual parameter values can be optionally fixed to reduce tuning complexity.
</p>


<h3>Value</h3>

<p>workflow object
</p>


<h3>Examples</h3>

<pre><code class="language-R">



iris %&gt;%
 framecleaner::create_dummies() -&gt; iris1

iris1 %&gt;%
 tidy_formula(target = Petal.Length) -&gt; petal_form

iris1 %&gt;%
 rsample::initial_split() -&gt; iris_split

iris_split %&gt;%
 rsample::analysis() -&gt; iris_train

iris_split %&gt;%
 rsample::assessment() -&gt; iris_val

## Not run: 
iris_train %&gt;%
 auto_tune_xgboost(formula = petal_form, n_iter = 10,
 parallel = FALSE, tune_method = "grid", mtry = .5) -&gt; xgb_tuned

xgb_tuned %&gt;%
 parsnip::fit(iris_train) %&gt;%
 parsnip::extract_fit_engine() -&gt; xgb_tuned_fit

xgb_tuned_fit %&gt;%
 tidy_predict(newdata = iris_val, form = petal_form) -&gt; iris_val1

## End(Not run)


</code></pre>


</div>