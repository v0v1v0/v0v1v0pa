<div class="container">

<table style="width: 100%;"><tr>
<td>ai</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>ai</h2>

<h3>Description</h3>

<p>Calculate Agreement Interval of Two Measurement Methods and quantify the agreement
</p>


<h3>Usage</h3>

<pre><code class="language-R">ai(x, y, lambda = 1, alpha = 0.05, clin.limit = NA)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A continous numeric vector from measurement method 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A continous numeric vector from measurement method 2, the same length as x.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Reliability ratio of x vs y. default 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Discordance rate to estimate confidence interval</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clin.limit</code></td>
<td>
<p>Clinically meaningful limit (optional)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This is the function to calculate agreement interval (confidence interval) of two continuous numerical vectors from two measurement methods on the same samples. Note that this function only works for scenario with two evaluators, for example, comparing the concordance between two evaluators. We are working on the scenario with more than two evaluators.
The two numerical vectors are <code>x</code> and <code>y</code>. It also provides commonly used measures based on index approaches,
for example, Pearson's correlation coefficient, the intraclass correlation coefficient (ICC),
the concordance correlation coefficient (Lin's CCC), and improved CCC (Liao's ICCC).
</p>


<h3>Value</h3>

<p>Function ai returns an object of class "ai".
</p>
<p>An object of class "ai" is a list containing the following components:
</p>
<p>alpha:   Alpha input for confidence interval estimates
</p>
<p>n: Sample size
</p>
<p>conf.level: Confidence level calculated from alpha
</p>
<p>lambda: Reliability ratio input of x vs y
</p>
<p>summaryStat: Summary statistics of input data
</p>
<p>sigma.e: Random error estimates
</p>
<p>indexEst: Agreement estimates (CI.) based on index approaches
</p>
<p>intervalEst: Agreement estimates (CI.) based on interval approaches
</p>
<p>biasEst: Bias estimate
</p>
<p>intercept: Intercept of linear regression line from measure error model
</p>
<p>slope: Slope of linear regression line from measure error model
</p>
<p>x.name: x variable name extracted from input, used for plotting
</p>
<p>y.name: y variable name extracted from input, used for plotting
</p>
<p>tolProb.cl: Tolrance probability calculated based on optional clinically meaningful limit
</p>
<p>k.cl: Number of discordance pairs based on optional clinically meaningful limit
</p>
<p>alpha.cl: Discordance rate based on clinically meaningful limit
</p>


<h3>Author(s)</h3>

<p>Jialin Xu, Jason Liao
</p>


<h3>References</h3>

<p>Luiz RR, Costa AJL, Kale PL, Werneck GL. Assessment of agreement of a quantitative variable: a new graphical approach. J Clin Epidemiol 2003; 56:963-7.
</p>
<p>Jason J. Z. Liao, Quantifying an Agreement Study, Int. J. Biostat. 2015; 11(1): 125-133
</p>
<p>Shrout, Patrick E. and Fleiss, Joseph L. Intraclass correlations: uses in assessing rater reliability. Psychological Bulletin, 1979, 86, 420-3428.
</p>
<p>Lin L-K., A Concordance Correlation Coefficient to Evaluate Reproducibility. Biometrics 1989; 45:255-68
</p>
<p>Liao JJ. An Improved Concordance Correlation Coefficient. Pharm Stat 2003; 2:253-61
</p>
<p>Nicole Jill-Marie Blackman, Reproducibility of Clinical Data I: Continuous Outcomes, Pharm Stat 2004; 3:99-108
</p>


<h3>Examples</h3>

<pre><code class="language-R">ai(x=1:4, y=c(1, 1, 2, 4))
a &lt;- c(1, 2, 3, 4, 7)
b &lt;- c(1, 3, 2, 5, 3)
ai(x=a, y=b)
ai(x=IPIA$Tomography, y=IPIA$Urography)
ai(x=IPIA$Tomography, y=IPIA$Urography, clin.limit=c(-15, 15))
</code></pre>


</div>