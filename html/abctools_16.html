<div class="container">

<table style="width: 100%;"><tr>
<td>nn.ent</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Works out entropy of a sample.</h2>

<h3>Description</h3>

<p>The function computes the k nearest neighbour sample entropy.</p>


<h3>Usage</h3>

<pre><code class="language-R">nn.ent(th, k=4)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>th</code></td>
<td>
<p>The sample from which to compute the entropy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>The order (number of neighbours) of the sample entropy calculation.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The sample entropy gives a measure of information in a (posterior) sample, or lack of it.</p>


<h3>Value</h3>

<p> The k nearest neighbour entropy from the sample.</p>


<h3>Warning</h3>

<p>For high-dimensional posterior samples, the <code>nn.ent</code> calculation is quite computationally intensive.
</p>


<h3>Author(s)</h3>

<p>Matt Nunes</p>


<h3>References</h3>

<p>Nunes, M. A. and Prangle, D. (2016) abctools: an R package for tuning
approximate Bayesian computation analyses. <em>The R Journal</em>
<b>7</b>, Issue 2, 189–205.<br><br>  
Singh, H. et al. (2003) Nearest neighbor estimates of entropy. <em>Am. J. Math. Man. Sci.</em>,<b>23</b>, 301–321.<br><br>
Shannon, C. E. and Weaver, W. (1948) A mathematical theory of communication. <em>Bell Syst. Tech. J.</em>, <b>27</b>, 379–423.
</p>


<h3>See Also</h3>

<p><code>mincrit</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# create a dummy sample to calculate an entropy measure:

theta&lt;-rnorm(10000)

nn.ent(theta)

</code></pre>


</div>