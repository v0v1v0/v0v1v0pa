<div class="container">

<table style="width: 100%;"><tr>
<td>train_tune_roberta_model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Function for training and fine-tuning a RoBERTa model</h2>

<h3>Description</h3>

<p>This function can be used to train or fine-tune a transformer
based on RoBERTa architecture with the help of the python libraries 'transformers',
'datasets', and 'tokenizers'.
</p>


<h3>Usage</h3>

<pre><code class="language-R">train_tune_roberta_model(
  ml_framework = aifeducation_config$get_framework(),
  output_dir,
  model_dir_path,
  raw_texts,
  p_mask = 0.15,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.03,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ml_framework</code></td>
<td>
<p><code>string</code> Framework to use for training and inference.
<code>ml_framework="tensorflow"</code> for 'tensorflow' and <code>ml_framework="pytorch"</code>
for 'pytorch'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_dir</code></td>
<td>
<p><code>string</code> Path to the directory where the final model
should be saved. If the directory does not exist, it will be created.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model_dir_path</code></td>
<td>
<p><code>string</code> Path to the directory where the original
model is stored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>raw_texts</code></td>
<td>
<p><code>vector</code> containing the raw texts for training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p_mask</code></td>
<td>
<p><code>double</code> Ratio determining the number of words/tokens for masking.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>val_size</code></td>
<td>
<p><code>double</code> Ratio determining the amount of token chunks used for
validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_epoch</code></td>
<td>
<p><code>int</code> Number of epochs for training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batch_size</code></td>
<td>
<p><code>int</code> Size of batches.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chunk_size</code></td>
<td>
<p><code>int</code> Size of every chunk for training.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>full_sequences_only</code></td>
<td>
<p><code>bool</code> <code>TRUE</code> for using only chunks
with a sequence length equal to <code>chunk_size</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_seq_len</code></td>
<td>
<p><code>int</code> Only relevant if <code>full_sequences_only=FALSE</code>.
Value determines the minimal sequence length for inclusion in training process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p><code>bool</code> Learning rate for adam optimizer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_workers</code></td>
<td>
<p><code>int</code> Number of workers. Only relevant if <code>ml_framework="tensorflow"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>multi_process</code></td>
<td>
<p><code>bool</code> <code>TRUE</code> if multiple processes should be activated.
Only relevant if <code>ml_framework="tensorflow"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_track</code></td>
<td>
<p><code>bool</code> If <code>TRUE</code> energy consumption is tracked
during training via the python library codecarbon.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_iso_code</code></td>
<td>
<p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable
must be set if sustainability should be tracked. A list can be found on
Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_region</code></td>
<td>
<p>Region within a country. Only available for USA and
Canada See the documentation of codecarbon for more information.
<a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_interval</code></td>
<td>
<p><code>integer</code> Interval in seconds for measuring power
usage.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p><code>bool</code> <code>TRUE</code> if information on the progress should be printed
to the console.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keras_trace</code></td>
<td>
<p><code>int</code> <code>keras_trace=0</code> does not print any
information about the training process from keras on the console.
<code>keras_trace=1</code> prints a progress bar. <code>keras_trace=2</code> prints
one line of information for every epoch. Only relevant if <code>ml_framework="tensorflow"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pytorch_trace</code></td>
<td>
<p><code>int</code> <code>pytorch_trace=0</code> does not print any
information about the training process from pytorch on the console.
<code>pytorch_trace=1</code> prints a progress bar.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pytorch_safetensors</code></td>
<td>
<p><code>bool</code> If <code>TRUE</code> a 'pytorch' model
is saved in safetensors format. If <code>FALSE</code> or 'safetensors' not available
it is saved in the standard pytorch format (.bin). Only relevant for pytorch models.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>This function does not return an object. Instead the trained or fine-tuned
model is saved to disk.
</p>


<h3>Note</h3>

<p>Pre-Trained models which can be fine-tuned with this function are available
at <a href="https://huggingface.co/">https://huggingface.co/</a>. New models can be created via the function
create_roberta_model.
</p>
<p>Training of this model makes use of dynamic masking.
</p>


<h3>References</h3>

<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,
Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). RoBERTa: A Robustly
Optimized BERT Pretraining Approach.
<a href="https://doi.org/10.48550/arXiv.1907.11692">doi:10.48550/arXiv.1907.11692</a>
</p>
<p>Hugging Face Documentation
<a href="https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaConfig">https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaConfig</a>
</p>


<h3>See Also</h3>

<p>Other Transformer: 
<code>create_bert_model()</code>,
<code>create_deberta_v2_model()</code>,
<code>create_funnel_model()</code>,
<code>create_longformer_model()</code>,
<code>create_roberta_model()</code>,
<code>train_tune_bert_model()</code>,
<code>train_tune_deberta_v2_model()</code>,
<code>train_tune_funnel_model()</code>,
<code>train_tune_longformer_model()</code>
</p>


</div>