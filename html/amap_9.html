<div class="container">

<table style="width: 100%;"><tr>
<td>hcluster</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Hierarchical Clustering</h2>

<h3>Description</h3>

<p>Hierarchical cluster analysis.
</p>


<h3>Usage</h3>

<pre><code class="language-R">hcluster(x, method = "euclidean", diag = FALSE, upper = FALSE,
         link = "complete", members = NULL, nbproc = 2,
         doubleprecision = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A numeric matrix of data, or an object that can be coerced to such a
matrix (such as a numeric vector or a data frame with all numeric
columns). Or an object of class "exprSet".
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the distance measure to be used. This must be one of
<code>"euclidean"</code>, <code>"maximum"</code>, <code>"manhattan"</code>,
<code>"canberra"</code>, <code>"binary"</code>, <code>"pearson"</code>,
<code>"abspearson"</code>,  <code>"correlation"</code>,
<code>"abscorrelation"</code>, <code>"spearman"</code> or <code>"kendall"</code>.
Any unambiguous substring can be given.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>diag</code></td>
<td>
<p>logical value indicating whether the diagonal of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>upper</code></td>
<td>
<p>logical value indicating whether the upper triangle of the
distance matrix should be printed by <code>print.dist</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>link</code></td>
<td>
<p>the agglomeration method to be used. This should
be (an unambiguous abbreviation of) one of
<code>"ward"</code>, <code>"single"</code>, <code>"complete"</code>,
<code>"average"</code>, <code>"mcquitty"</code>, <code>"median"</code> or
<code>"centroid"</code>,<code>"centroid2"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>members</code></td>
<td>
<p><code>NULL</code> or a vector with length size of <code>d</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nbproc</code></td>
<td>
<p>integer, number of subprocess for parallelization [Linux
&amp; Mac only]</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>doubleprecision</code></td>
<td>
<p>True: use of double precision for distance
matrix computation; False: use simple precision</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function is a mix of function <code>hclust</code> and function
<code>dist</code>. <code>hcluster(x, method = "euclidean",link = "complete")
     = hclust(dist(x, method = "euclidean"),method = "complete"))</code>   
It use twice less memory, as it doesn't store distance matrix.
</p>
<p>For more details, see documentation of <code>hclust</code> and <code>Dist</code>.
</p>


<h3>Value</h3>

<p>An object of class <b>hclust</b> which describes the
tree produced by the clustering process.
The object is a list with components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>merge</code></td>
<td>
<p>an <code class="reqn">n-1</code> by 2 matrix.
Row <code class="reqn">i</code> of <code>merge</code> describes the merging of clusters
at step <code class="reqn">i</code> of the clustering.
If an element <code class="reqn">j</code> in the row is negative,
then observation <code class="reqn">-j</code> was merged at this stage.
If <code class="reqn">j</code> is positive then the merge
was with the cluster formed at the (earlier) stage <code class="reqn">j</code>
of the algorithm.
Thus negative entries in <code>merge</code> indicate agglomerations
of singletons, and positive entries indicate agglomerations
of non-singletons.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>height</code></td>
<td>
<p>a set of <code class="reqn">n-1</code> non-decreasing real values.
The clustering <em>height</em>: that is, the value of
the criterion associated with the clustering
<code>method</code> for the particular agglomeration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>order</code></td>
<td>
<p>a vector giving the permutation of the original
observations suitable for plotting, in the sense that a cluster
plot using this ordering and matrix <code>merge</code> will not have
crossings of the branches.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>labels</code></td>
<td>
<p>labels for each of the objects being clustered.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>the call which produced the result.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>the cluster method that has been used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dist.method</code></td>
<td>
<p>the distance that has been used to create <code>d</code>
(only returned if the distance object has a <code>"method"</code>
attribute).</p>
</td>
</tr>
</table>
<p>There is a <code>print</code> and a <code>plot</code> method for
<code>hclust</code> objects.
The <code>plclust()</code> function is basically the same as the plot method,
<code>plot.hclust</code>, primarily for back compatibility with S-plus. Its
extra arguments are not yet implemented.
</p>


<h3>Note</h3>

<p>Multi-thread (parallelisation) is disable on Windows.</p>


<h3>Author(s)</h3>

<p>The <code>hcluster</code> function is based on C code adapted from Cran
Fortran routine
by Antoine Lucas.
</p>


<h3>References</h3>

<p>Antoine Lucas and Sylvain Jasson, <em>Using amap and ctc Packages
for Huge Clustering</em>, R News, 2006, vol 6, issue 5 pages 58-60.
</p>


<h3>See Also</h3>

<p><code>Dist</code>,
<code>hclust</code>, <code>kmeans</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(USArrests)
hc &lt;- hcluster(USArrests,link = "ave")
plot(hc)
plot(hc, hang = -1)

## Do the same with centroid clustering and squared Euclidean distance,
## cut the tree into ten clusters and reconstruct the upper part of the
## tree from the cluster centers.
hc &lt;- hclust(dist(USArrests)^2, "cen")
memb &lt;- cutree(hc, k = 10)
cent &lt;- NULL
for(k in 1:10){
  cent &lt;- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
}
hc1 &lt;- hclust(dist(cent)^2, method = "cen", members = table(memb))
opar &lt;- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)


## other combinaison are possible

hc &lt;- hcluster(USArrests,method = "euc",link = "ward", nbproc= 1,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "max",link = "single", nbproc= 2,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "man",link = "complete", nbproc= 1,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "can",link = "average", nbproc= 2,
doubleprecision = TRUE)
hc &lt;- hcluster(USArrests,method = "bin",link = "mcquitty", nbproc= 1,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "pea",link = "median", nbproc= 2,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "abspea",link = "median", nbproc= 2,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "cor",link = "centroid", nbproc= 1,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "abscor",link = "centroid", nbproc= 1,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "spe",link = "complete", nbproc= 2,
doubleprecision = FALSE)
hc &lt;- hcluster(USArrests,method = "ken",link = "complete", nbproc= 2,
doubleprecision = FALSE)



</code></pre>


</div>