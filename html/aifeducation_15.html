<div class="container">

<table style="width: 100%;"><tr>
<td>create_longformer_model</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Function for creating a new transformer based on Longformer</h2>

<h3>Description</h3>

<p>This function creates a transformer configuration based on the Longformer base architecture
and a vocabulary based on Byte-Pair Encoding (BPE) tokenizer by using
the python libraries 'transformers' and 'tokenizers'.
</p>


<h3>Usage</h3>

<pre><code class="language-R">create_longformer_model(
  ml_framework = aifeducation_config$get_framework,
  model_dir,
  vocab_raw_texts = NULL,
  vocab_size = 30522,
  add_prefix_space = FALSE,
  trim_offsets = TRUE,
  max_position_embeddings = 512,
  hidden_size = 768,
  num_hidden_layer = 12,
  num_attention_heads = 12,
  intermediate_size = 3072,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  attention_window = 512,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ml_framework</code></td>
<td>
<p><code>string</code> Framework to use for training and inference.
<code>ml_framework="tensorflow"</code> for 'tensorflow' and <code>ml_framework="pytorch"</code>
for 'pytorch'.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model_dir</code></td>
<td>
<p><code>string</code> Path to the directory where the model should be saved.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vocab_raw_texts</code></td>
<td>
<p><code>vector</code> containing the raw texts for creating the
vocabulary.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vocab_size</code></td>
<td>
<p><code>int</code> Size of the vocabulary.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>add_prefix_space</code></td>
<td>
<p><code>bool</code> <code>TRUE</code> if an additional space should be insert
to the leading words.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trim_offsets</code></td>
<td>
<p><code>bool</code> <code>TRUE</code> trims the whitespaces from the produced offsets.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_position_embeddings</code></td>
<td>
<p><code>int</code> Number of maximal position embeddings. This parameter
also determines the maximum length of a sequence which can be processed with the model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_size</code></td>
<td>
<p><code>int</code> Number of neurons in each layer. This parameter determines the
dimensionality of the resulting text embedding.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_hidden_layer</code></td>
<td>
<p><code>int</code> Number of hidden layers.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_attention_heads</code></td>
<td>
<p><code>int</code> Number of attention heads.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intermediate_size</code></td>
<td>
<p><code>int</code> Number of neurons in the intermediate layer of
the attention mechanism.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_act</code></td>
<td>
<p><code>string</code> name of the activation function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_dropout_prob</code></td>
<td>
<p><code>double</code> Ratio of dropout</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>attention_probs_dropout_prob</code></td>
<td>
<p><code>double</code> Ratio of dropout for attention
probabilities.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>attention_window</code></td>
<td>
<p><code>int</code> Size of the window around each token for
attention mechanism in every layer.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_track</code></td>
<td>
<p><code>bool</code> If <code>TRUE</code> energy consumption is tracked
during training via the python library codecarbon.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_iso_code</code></td>
<td>
<p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable
must be set if sustainability should be tracked. A list can be found on
Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_region</code></td>
<td>
<p>Region within a country. Only available for USA and
Canada See the documentation of codecarbon for more information.
<a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sustain_interval</code></td>
<td>
<p><code>integer</code> Interval in seconds for measuring power
usage.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trace</code></td>
<td>
<p><code>bool</code> <code>TRUE</code> if information about the progress should be
printed to the console.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pytorch_safetensors</code></td>
<td>
<p><code>bool</code> If <code>TRUE</code> a 'pytorch' model
is saved in safetensors format. If <code>FALSE</code> or 'safetensors' not available
it is saved in the standard pytorch format (.bin). Only relevant for pytorch models.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>This function does not return an object. Instead the configuration
and the vocabulary of the new model are saved on disk.
</p>


<h3>Note</h3>

<p>To train the model, pass the directory of the model to the function
train_tune_longformer_model.
</p>


<h3>References</h3>

<p>Beltagy, I., Peters, M. E., &amp; Cohan, A. (2020). Longformer: The
Long-Document Transformer. <a href="https://doi.org/10.48550/arXiv.2004.05150">doi:10.48550/arXiv.2004.05150</a>
</p>
<p>Hugging Face Documentation
<a href="https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerConfig">https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerConfig</a>
</p>


<h3>See Also</h3>

<p>Other Transformer: 
<code>create_bert_model()</code>,
<code>create_deberta_v2_model()</code>,
<code>create_funnel_model()</code>,
<code>create_roberta_model()</code>,
<code>train_tune_bert_model()</code>,
<code>train_tune_deberta_v2_model()</code>,
<code>train_tune_funnel_model()</code>,
<code>train_tune_longformer_model()</code>,
<code>train_tune_roberta_model()</code>
</p>


</div>