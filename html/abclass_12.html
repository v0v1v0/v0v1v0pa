<div class="container">

<table style="width: 100%;"><tr>
<td>supclass</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multi-Category Classifiers with Sup-Norm Regularization</h2>

<h3>Description</h3>

<p>Experimental implementations of multi-category classifiers with sup-norm
penalties proposed by Zhang, et al. (2008) and Li &amp; Zhang (2021).
</p>


<h3>Usage</h3>

<pre><code class="language-R">supclass(
  x,
  y,
  model = c("logistic", "psvm", "svm"),
  penalty = c("lasso", "scad"),
  start = NULL,
  control = list(),
  ...
)

supclass.control(
  lambda = 0.1,
  adaptive_weight = NULL,
  scad_a = 3.7,
  maxit = 50,
  epsilon = 1e-04,
  shrinkage = 1e-04,
  warm_start = TRUE,
  standardize = TRUE,
  verbose = 0L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A numeric matrix representing the design matrix.  No missing valus
are allowed.  The coefficient estimates for constant columns will be
zero.  Thus, one should set the argument <code>intercept</code> to <code>TRUE</code>
to include an intercept term instead of adding an all-one column to
<code>x</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>An integer vector, a character vector, or a factor vector
representing the response label.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A charactor vector specifying the classification model.  The
available options are <code>"logistic"</code> for multi-nomial logistic
regression model, <code>"psvm"</code> for proximal support vector machine
(PSVM), <code>"svm"</code> for multi-category support vector machine.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>A charactor vector specifying the penalty function for the
sup-norms.  The available options are <code>"lasso"</code> for sup-norm
regularization proposed by Zhang et al. (2008) and <code>"scad"</code> for
supSCAD regularization proposed by Li &amp; Zhang (2021).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>start</code></td>
<td>
<p>A numeric matrix representing the starting values for the
quadratic approximation procedure behind the scene.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>A list with named elements.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Optional control parameters passed to the
<code>supclass.control()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>A numeric vector specifying the tuning parameter
<em>lambda</em>.  The default value is <code>0.1</code>.  Users should tune this
parameter for a better model fit.  The specified lambda will be sorted
in decreasing order internally and only the unique values will be kept.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adaptive_weight</code></td>
<td>
<p>A numeric vector or matrix representing the adaptive
penalty weights.  The default value is <code>NULL</code> for equal weights.
Zhang, et al. (2008) proposed two ways to employ the adaptive weights.
The first approach applies the weights to the sup-norm of coefficient
estimates, while the second approach applies element-wise multiplication
to the weights and coefficient estimates inside the sup-norms.  The
first or second approach will be applied if a numeric vector or matrix
is specified, respectively.  The adaptive weights are supported for
lasso penalty only.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scad_a</code></td>
<td>
<p>A positive number specifying the tuning parameter <em>a</em> in
the SCAD penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>A positive integer specifying the maximum number of iteration.
The default value is <code>50</code> as suggested in Li &amp; Zhang (2021).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>A positive number specifying the relative tolerance that
determines convergence.  The default value is <code>1e-4</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>shrinkage</code></td>
<td>
<p>A nonnegative tolerance to shrink estimates with sup-norm
close enough to zero (within the specified tolerance) to zeros.  The
default value is <code>1e-4</code>.  ## @param ridge_lambda The tuning
parameter lambda of the ridge penalty used to ## set the (first set of)
starting values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>warm_start</code></td>
<td>
<p>A logical value indicating if the estimates from last
lambda should be used as the starting values for the next lambda.  If
<code>FALSE</code>, the user-specified starting values will be used instead.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>A logical value indicating if a standardization procedure
should be performed so that each column of the design matrix has mean
zero and standardization</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>A nonnegative integer specifying if the estimation procedure
is allowed to print out intermediate steps/results.  The default value
is <code>0</code> for silent estimation procedure.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For the multinomial logistic model or the proximal SVM model, this function
utilizes the function <code>quadprog::solve.QP()</code> to solve the equivalent
quadratic problem; For the multi-class SVM, this function utilizes GNU GLPK
to solve the equivalent linear programming problem via the package Rglpk.
It is recommended to use a recent version of GLPK.
</p>


<h3>References</h3>

<p>Zhang, H. H., Liu, Y., Wu, Y., &amp; Zhu, J. (2008). Variable selection for the
multicategory SVM via adaptive sup-norm regularization. <em>Electronic
Journal of Statistics</em>, 2, 149–167.
</p>
<p>Li, N., &amp; Zhang, H. H. (2021). Sparse learning with non-convex penalty in
multi-classification. <em>Journal of Data Science</em>, 19(1), 56–74.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(abclass)
set.seed(123)

## toy examples for demonstration purpose
## reference: example 1 in Zhang and Liu (2014)
ntrain &lt;- 100 # size of training set
ntest &lt;- 1000 # size of testing set
p0 &lt;- 2       # number of actual predictors
p1 &lt;- 2       # number of random predictors
k &lt;- 3        # number of categories

n &lt;- ntrain + ntest; p &lt;- p0 + p1
train_idx &lt;- seq_len(ntrain)
y &lt;- sample(k, size = n, replace = TRUE)         # response
mu &lt;- matrix(rnorm(p0 * k), nrow = k, ncol = p0) # mean vector
## normalize the mean vector so that they are distributed on the unit circle
mu &lt;- mu / apply(mu, 1, function(a) sqrt(sum(a ^ 2)))
x0 &lt;- t(sapply(y, function(i) rnorm(p0, mean = mu[i, ], sd = 0.25)))
x1 &lt;- matrix(rnorm(p1 * n, sd = 0.3), nrow = n, ncol = p1)
x &lt;- cbind(x0, x1)
train_x &lt;- x[train_idx, ]
test_x &lt;- x[- train_idx, ]
y &lt;- factor(paste0("label_", y))
train_y &lt;- y[train_idx]
test_y &lt;- y[- train_idx]

## regularization with the supnorm lasso penalty
options("mc.cores" = 1)
model &lt;- supclass(train_x, train_y, model = "psvm", penalty = "lasso")
pred &lt;- predict(model, test_x)
table(test_y, pred)
mean(test_y == pred) # accuracy
</code></pre>


</div>