<div class="container">

<table style="width: 100%;"><tr>
<td>h2o_train</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Model wrappers for h2o</h2>

<h3>Description</h3>

<p>Basic model wrappers for h2o model functions that include data conversion,
seed configuration, and so on.
</p>


<h3>Usage</h3>

<pre><code class="language-R">h2o_train(
  x,
  y,
  model,
  weights = NULL,
  validation = NULL,
  save_data = FALSE,
  ...
)

h2o_train_rf(x, y, ntrees = 50, mtries = -1, min_rows = 1, ...)

h2o_train_xgboost(
  x,
  y,
  ntrees = 50,
  max_depth = 6,
  min_rows = 1,
  learn_rate = 0.3,
  sample_rate = 1,
  col_sample_rate = 1,
  min_split_improvement = 0,
  stopping_rounds = 0,
  validation = NULL,
  ...
)

h2o_train_gbm(
  x,
  y,
  ntrees = 50,
  max_depth = 6,
  min_rows = 1,
  learn_rate = 0.3,
  sample_rate = 1,
  col_sample_rate = 1,
  min_split_improvement = 0,
  stopping_rounds = 0,
  ...
)

h2o_train_glm(x, y, lambda = NULL, alpha = NULL, ...)

h2o_train_nb(x, y, laplace = 0, ...)

h2o_train_mlp(
  x,
  y,
  hidden = 200,
  l2 = 0,
  hidden_dropout_ratios = 0,
  epochs = 10,
  activation = "Rectifier",
  validation = NULL,
  ...
)

h2o_train_rule(
  x,
  y,
  rule_generation_ntrees = 50,
  max_rule_length = 5,
  lambda = NULL,
  ...
)

h2o_train_auto(x, y, verbosity = NULL, save_data = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A data frame of predictors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A vector of outcomes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A character string for the model. Current selections are
<code>"automl"</code>, <code>"randomForest"</code>, <code>"xgboost"</code>, <code>"gbm"</code>, <code>"glm"</code>, <code>"deeplearning"</code>,
<code>"rulefit"</code> and <code>"naiveBayes"</code>. Use <code>h2o_xgboost_available()</code> to see
if xgboost can be used on your OS/h2o server.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>A numeric vector of case weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validation</code></td>
<td>
<p>An integer between 0 and 1 specifying the <em>proportion</em> of
the data reserved as validation set. This is used by h2o for performance
assessment and potential early stopping. Default to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>save_data</code></td>
<td>
<p>A logical for whether training data should be saved on
the h2o server, set this to <code>TRUE</code> for AutoML models that needs to be
re-fitted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other options to pass to the h2o model functions (e.g.,
<code>h2o::h2o.randomForest()</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ntrees</code></td>
<td>
<p>Number of trees. Defaults to 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mtries</code></td>
<td>
<p>Number of variables randomly sampled as candidates at each split. If set to -1, defaults to sqrt{p} for
classification and p/3 for regression (where p is the # of predictors Defaults to -1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_rows</code></td>
<td>
<p>Fewest allowed (weighted) observations in a leaf. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_depth</code></td>
<td>
<p>Maximum tree depth (0 for unlimited). Defaults to 20.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learn_rate</code></td>
<td>
<p>(same as eta) Learning rate (from 0.0 to 1.0) Defaults to 0.3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_rate</code></td>
<td>
<p>Row sample rate per tree (from 0.0 to 1.0) Defaults to 0.632.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>col_sample_rate</code></td>
<td>
<p>(same as colsample_bylevel) Column sample rate (from 0.0 to 1.0) Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_split_improvement</code></td>
<td>
<p>Minimum relative improvement in squared error reduction for a split to happen Defaults to 1e-05.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stopping_rounds</code></td>
<td>
<p>Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the
stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable) Defaults to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Regularization strength</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Distribution of regularization between the L1 (Lasso) and L2 (Ridge) penalties. A value of 1 for alpha
represents Lasso regression, a value of 0 produces Ridge regression, and anything in between specifies the
amount of mixing between the two. Default value of alpha is 0 when SOLVER = 'L-BFGS'; 0.5 otherwise.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>laplace</code></td>
<td>
<p>Laplace smoothing parameter Defaults to 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden</code></td>
<td>
<p>Hidden layer sizes (e.g. [100, 100]). Defaults to c(200, 200).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l2</code></td>
<td>
<p>L2 regularization (can add stability and improve generalization, causes many weights to be small. Defaults to
0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hidden_dropout_ratios</code></td>
<td>
<p>Hidden layer dropout ratios (can improve generalization), specify one value per hidden layer, defaults to 0.5.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epochs</code></td>
<td>
<p>How many times the dataset should be iterated (streamed), can be fractional. Defaults to 10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>activation</code></td>
<td>
<p>Activation function. Must be one of: "Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout", "Maxout",
"MaxoutWithDropout". Defaults to Rectifier.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rule_generation_ntrees</code></td>
<td>
<p>Specifies the number of trees to build in the tree model. Defaults to 50. Defaults to 50.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_rule_length</code></td>
<td>
<p>Maximum length of rules. Defaults to 3.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbosity</code></td>
<td>
<p>Verbosity of the backend messages printed during training;
Must be one of NULL (live log disabled), "debug", "info", "warn", "error".
Defaults to NULL.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>An h2o model object.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# start with h2o::h2o.init()
if (h2o_running()) {
 # -------------------------------------------------------------------------
 # Using the model wrappers:
 h2o_train_glm(mtcars[, -1], mtcars$mpg)

 # -------------------------------------------------------------------------
 # using parsnip:

 spec &lt;-
   rand_forest(mtry = 3, trees = 500) %&gt;%
   set_engine("h2o") %&gt;%
   set_mode("regression")

 set.seed(1)
 mod &lt;- fit(spec, mpg ~ ., data = mtcars)
 mod

 predict(mod, head(mtcars))
}

</code></pre>


</div>