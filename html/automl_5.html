<div class="container">

<table style="width: 100%;"><tr>
<td>hpar</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Deep Neural Net parameters and hyperparameters</h2>

<h3>Description</h3>

<p>List of Neural Network parameters and hyperparameters to train with gradient descent or particle swarm optimization<br>
Not mandatory (the list is preset and all arguments are initialized with default value) but it is advisable to adjust some important arguments for performance reasons (including processing time)
</p>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>modexec</code></td>
<td>
 <p>‘trainwgrad’ (the default value) to train with gradient descent (suitable for all volume of data)<br>
‘trainwpso’ to train using Particle Swarm Optimization, each particle represents a set of neural network weights (CAUTION: suitable for low volume of data, time consuming for medium to large volume of data)</p>
</td>
</tr></table>
<p><br></p>
<p><em>Below specific arguments to ‘trainwgrad’ execution mode</em>
</p>
<table>
<tr style="vertical-align: top;">
<td><code>learningrate</code></td>
<td>
<p> learningrate alpha (default value 0.001)<br>
#tuning priority 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta1</code></td>
<td>
<p> see below</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta2</code></td>
<td>
 <p>‘Momentum’ if beta1 different from 0 and beta2 equal 0 )<br>
‘RMSprop’ if beta1 equal 0 and beta2 different from 0<br>
‘adam optimization’ if beta1 different from 0 and beta2 different from 0 (default)<br>
(default value beta1 equal 0.9 and beta2 equal 0.999)<br>
#tuning priority 2</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lrdecayrate</code></td>
<td>
<p> learning rate decay value (default value 0, no learning rate decay, 1e-6 should be a good value to start with)<br>
#tuning priority 4</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chkgradevery</code></td>
<td>
<p> epoch interval to run gradient check function (default value 0, for debug only)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>chkgradepsilon</code></td>
<td>
<p> epsilon value for derivative calculations and threshold test in gradient check function (default 0.0000001)</p>
</td>
</tr>
</table>
<p><br></p>
<p><em>Below specific arguments to ‘trainwpso’ execution mode</em>
</p>
<table>
<tr style="vertical-align: top;">
<td><code>psoxxx</code></td>
<td>
<p> see pso for PSO specific arguments details</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>costcustformul</code></td>
<td>
<p> custom cost formula (default ‘’, no custom cost function)<br>
standard input variables: yhat (prediction), y (target actual value)<br>
custom input variables: any variable declared in hpar may be used via alias mydl (ie: hpar(list = (foo = 1.5)) will be used in custom cost formula as mydl$foo))<br>
result: J<br>
see ‘automl_train_manual’ example using Mean Average Percentage Error cost function<br>
nb: X and Y matrices used as input into automl_train_manual or automl_train_manual functions are transposed (features in rows and cases in columns)</p>
</td>
</tr>
</table>
<p><br></p>
<p><em>Below arguments for both execution modes</em>
</p>
<table>
<tr style="vertical-align: top;">
<td><code>numiterations</code></td>
<td>
<p> number of training epochs (default value 50))<br>
#tuning priority 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p> seed for reproductibility (default 4)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>minibatchsize</code></td>
<td>
<p> mini batch size, 2 to the power 0 for stochastic gradient descent (default 2 to the power 5)
#tuning priority 3</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layersshape</code></td>
<td>
<p> number of nodes per layer, each nodes number initialize a hidden layer<br>
output layer nodes number, may be left to 0 it will be automatically set by Y matrix shape<br>
default value one hidden layer with 10 nodes: c(10, 0)<br>
#tuning priority 4</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layersacttype</code></td>
<td>
<p> activation function for each layer; ‘linear’ for no activation or ‘sigmoid’, ‘relu’ or ‘reluleaky’ or ‘tanh’ or ‘softmax’ (softmax for output layer only supported in trainwpso exec mode)<br>
output layer activation function may be left to ‘’, default value ‘linear’ for regression, ‘sigmoid’ for classification<br>
nb: layersacttype parameter vector must have same length as layersshape parameter vector<br>
default value c(‘relu’, ‘’)<br>
#tuning priority 4</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>layersdropoprob</code></td>
<td>
<p> drop out probability for each layer, continuous value from 0 to less than 1 (give the percentage of matrix weight values to drop out randomly)<br>
nb: layersdropoprob parameter vector must have same length as layersshape parameter vector<br>
default value no drop out: c(0, 0)<br>
#tuning priority for regularization</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>printcostevery</code></td>
<td>
<p> epoch interval to test and print costs (train and cross validation cost: default value 10, for 1 test every 10 epochs)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>testcvsize</code></td>
<td>
<p> size of cross validation sample, 0 for no cross validation sample (default 10, for 10 percent)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>testgainunder</code></td>
<td>
<p> threshold to stop the training if the gain between last train or cross validation cost is smaller than the threshold, 0 for no stop test (default 0.000001)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>costtype</code></td>
<td>
<p> cost type function name ‘mse’ or ‘crossentropy’ or ‘custom’<br>
‘mse’ for Mean Squared Error, set automatically for continuous target type (‘mape’ Mean Absolute Percentage Error may be specified)<br>
‘crossentropy’ set automatically for binary target type<br>
‘custom’ set automatically if ‘costcustformul’ different from ‘’
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p> regularization term added to cost function (default value 0, no regularization)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>batchnor_mom</code></td>
<td>
<p> batch normalization momentum for j and B (default 0, no batch normalization, may be set to 0.9 for deep neural net)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsil</code></td>
<td>
<p> epsilon the low value to avoid dividing by 0 or log(0) in cost function, etc ... (default value 1e-12)<br></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p> to display or not the costs and the shapes (default TRUE)</p>
</td>
</tr>
</table>
<p><br></p>
<p><em>back to  automl_train, automl_train_manual</em>
</p>


<h3>See Also</h3>

<p>Deep Learning specialization from Andrew NG on Coursera
</p>


</div>