<div class="container">

<table style="width: 100%;"><tr>
<td>cv.nfeaturesLDA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Cross-validation to find the optimum number of features (variables) in LDA</h2>

<h3>Description</h3>

<p>This function provids an illustration of the process of finding out the
optimum number of variables using k-fold cross-validation in a linear
discriminant analysis (LDA).
</p>


<h3>Usage</h3>

<pre><code class="language-R">cv.nfeaturesLDA(
  data = matrix(rnorm(600), 60),
  cl = gl(3, 20),
  k = 5,
  cex.rg = c(0.5, 3),
  col.av = c("blue", "red"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>a data matrix containg the predictors in columns</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cl</code></td>
<td>
<p>a factor indicating the classification of the rows of <code>data</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>the number of folds</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cex.rg</code></td>
<td>
<p>the range of the magnification to be used to the points in the
plot</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>col.av</code></td>
<td>
<p>the two colors used to respectively denote rates of correct
predictions in the i-th fold and the average rates for all k folds</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>arguments passed to <code>points</code> to draw the
points which denote the correct rate</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For a classification problem, usually we wish to use as less variables as
possible because of difficulties brought by the high dimension.
</p>
<p>The selection procedure is like this:
</p>

<ul>
<li>
<p> Split the whole data randomly into <code class="reqn">k</code> folds:
</p>

<ul>
<li>
<p> For the number of features <code class="reqn">g = 1, 2, \cdots, g_{max}</code>, choose <code class="reqn">g</code> features that have the largest discriminatory
power (measured by the F-statistic in ANOVA):
</p>

<ul><li>
<p> For the fold <code class="reqn">i</code> (<code class="reqn">i = 1, 2, \cdots, k</code>):
</p>

<ul><li>
<p>Train a LDA model without the <code class="reqn">i</code>-th fold data, and predict with the
<code class="reqn">i</code>-th fold for a proportion of correct predictions
<code class="reqn">p_{gi}</code>;
</p>
</li></ul>
</li></ul>
</li>
<li>
<p> Average the <code class="reqn">k</code> proportions to get the correct rate <code class="reqn">p_g</code>;
</p>
</li>
</ul>
</li>
<li>
<p> Determine the optimum number of features with the largest <code class="reqn">p</code>.
</p>
</li>
</ul>
<p>Note that <code class="reqn">g_{max}</code> is set by <code>ani.options('nmax')</code> (i.e. the
maximum number of features we want to choose).
</p>


<h3>Value</h3>

<p>A list containing </p>
<table>
<tr style="vertical-align: top;">
<td><code>accuracy </code></td>
<td>
<p>a matrix in which the element in
the i-th row and j-th column is the rate of correct predictions based on
LDA, i.e. build a LDA model with j variables and predict with data in the
i-th fold (the test set) </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>optimum </code></td>
<td>
<p>the optimum number of features
based on the cross-validation</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Yihui Xie &lt;<a href="https://yihui.org/">https://yihui.org/</a>&gt;
</p>


<h3>References</h3>

<p>Examples at <a href="https://yihui.org/animation/example/cv-nfeatureslda/">https://yihui.org/animation/example/cv-nfeatureslda/</a>
</p>
<p>Maindonald J, Braun J (2007). <em>Data Analysis and Graphics
Using R - An Example-Based Approach</em>. Cambridge University Press, 2nd
edition. pp. 400
</p>


<h3>See Also</h3>

<p><code>kfcv</code>, <code>cv.ani</code>, <code>lda</code>
</p>


</div>