<div class="container">

<table style="width: 100%;"><tr>
<td>ASDA</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Accelerated Sparse Discriminant Analysis</h2>

<h3>Description</h3>

<p>Applies accelerated proximal gradient algorithm, proximal gradient algorithm
or alternating direction methods of multipliers algorithm to
the optimal scoring formulation of sparse discriminant analysis proposed
by Clemmensen et al. 2011.
</p>
<p style="text-align: center;"><code class="reqn">argmin{|(Y_t\theta-X_t\beta)|_2^2 + t|\beta|_1 + \lambda|\beta|_2^2}</code>
</p>



<h3>Usage</h3>

<pre><code class="language-R">ASDA(Xt, ...)

## Default S3 method:
ASDA(
  Xt,
  Yt,
  Om = diag(p),
  gam = 0.001,
  lam = 1e-06,
  q = K - 1,
  method = "SDAAP",
  control = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Xt</code></td>
<td>
<p>n by p data matrix, (can also be a data.frame that can be coerced to a matrix)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional arguments for <code>lda</code> function in package MASS.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Yt</code></td>
<td>
<p>n by K matrix of indicator variables (Yij = 1 if i in class j).
This will later be changed to handle factor variables as well.
Each observation belongs in a single class, so for a given row/observation,
only one element is 1 and the rest is 0.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Om</code></td>
<td>
<p>p by p parameter matrix Omega in generalized elastic net penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gam</code></td>
<td>
<p>Regularization parameter for elastic net penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lam</code></td>
<td>
<p>Regularization parameter for l1 penalty, must be greater than zero.
If cross-validation is used (<code>CV = TRUE</code>) then this must be a vector
of length greater than one.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>q</code></td>
<td>
<p>Desired number of discriminant vectors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>This parameter selects which optimization method to use.
It is specified as a character vector which can be one of the three values
</p>

<dl>
<dt><code>SDAP</code></dt>
<dd>
<p>Proximal gradient algorithm.</p>
</dd>
<dt><code>SDAAP</code></dt>
<dd>
<p>Accelerated proximal gradient algorithm.</p>
</dd>
<dt><code>SDAD</code></dt>
<dd>
<p>Alternating directions method of multipliers algorithm.</p>
</dd>
</dl>
<p>Note that further parameters are passed to the function in the argument <code>control</code>,
which is a list with named components.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p>List of control arguments. See Details.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The control list contains the following entries to further tune the
algorithms.
</p>

<dl>
<dt><code>PGsteps</code></dt>
<dd>
<p>Maximum number if inner proximal gradient/ADMM
algorithm for finding beta. Default value is 1000.</p>
</dd>
<dt><code>PGtol</code></dt>
<dd>
<p>Stopping tolerance for inner method. If the method is <code>SDAD</code>,
then this must be a vector of two values, absolute (first element) and relative
tolerance (second element). Default value is 1e-5 for both absolute and
relative tolerances.</p>
</dd>
<dt><code>maxits</code></dt>
<dd>
<p>Number of iterations to run. Default value is 250.</p>
</dd>
<dt><code>tol</code></dt>
<dd>
<p>Stopping tolerance. Default value is 1e-3.</p>
</dd>
<dt><code>mu</code></dt>
<dd>
<p>Penalty parameter for augmented Lagrangian term,
must be greater than zero and only needs to be specified when using
method <code>SDAD</code>. Default value is 1.</p>
</dd>
<dt><code>CV</code></dt>
<dd>
<p>Logical value which is <code>TRUE</code> if cross validation is supposed to be
performed. If cross-validation is performed, then lam should be specified as
a vector containing the regularization values to be tested. Default value is <code>FALSE</code>.</p>
</dd>
<dt><code>folds</code></dt>
<dd>
<p>Integer determining the number of folds in cross-validation. Not needed
if CV is not specified. Default value is 5.</p>
</dd>
<dt><code>feat</code></dt>
<dd>
<p>Maximum fraction of nonzero features desired in validation scheme. Not needed
if CV is not specified. Default value is 0.15.</p>
</dd>
<dt><code>quiet</code></dt>
<dd>
<p>Set to <code>FALSE</code> if status updates are supposed to be printed to the R console.
Default value is <code>TRUE</code>. Note that this triggers a lot of printing to the console.</p>
</dd>
<dt><code>ordinal</code></dt>
<dd>
<p>Set to <code>TRUE</code> if the labels are ordinal. Only available for methods
<code>SDAAP</code> and <code>SDAD</code>.</p>
</dd>
<dt><code>initTheta</code></dt>
<dd>
<p>Option to set the initial theta vector, by default it is a vector of all ones
for the first theta.</p>
</dd>
<dt><code>bt</code></dt>
<dd>
<p>Logical indicating whether backtracking should be used, only applies to
the Proximal Gradient based methods. By default, backtracking is not used.</p>
</dd>
<dt><code>L</code></dt>
<dd>
<p>Initial estimate for Lipshitz constant used for backtracking. Default value is 0.25.</p>
</dd>
<dt><code>eta</code></dt>
<dd>
<p>Scalar for Lipshitz constant. Default value is 1.25.</p>
</dd>
<dt><code>rankRed</code></dt>
<dd>
<p>Boolean indicating whether Om is factorized, such that R^t*R=Om,
currently only applicable for accelerated proximal gradient.</p>
</dd>
</dl>
<h3>Value</h3>

<p><code>ASDA</code> returns an object of <code>class</code> "<code>ASDA</code>" including a list
with the following named components:
</p>

<dl>
<dt><code>call</code></dt>
<dd>
<p>The matched call.</p>
</dd>
<dt><code>B</code></dt>
<dd>
<p>p by q matrix of discriminant vectors, i.e. sparse loadings.</p>
</dd>
<dt><code>Q</code></dt>
<dd>
<p>K by q matrix of scoring vectors, i.e. optimal scores.</p>
</dd>
<dt><code>varNames</code></dt>
<dd>
<p>Names of the predictors used, i.e. column names of Xt.</p>
</dd>
<dt><code>origP</code></dt>
<dd>
<p>Number of variables in Xt.</p>
</dd>
<dt><code>fit</code></dt>
<dd>
<p>Output from function <code>lda</code> on projected data.
This is <code>NULL</code> the trivial solution is found, i.e. B is all zeroes. Use
lower values of <code>lam</code> if that is the case.</p>
</dd>
<dt><code>classes</code></dt>
<dd>
<p>The classes in Yt.</p>
</dd>
<dt><code>lambda</code></dt>
<dd>
<p>The lambda/<code>lam</code> used, best value found by cross-
validation if <code>CV</code> is <code>TRUE</code>.</p>
</dd>
</dl>
<p><code>NULL</code>
</p>


<h3>Note</h3>

<p>The input matrix Xt should be normalized, i.e. each column corresponding to
a variable should have its mean subtracted and scaled to unit length. The functions
<code>normalize</code> and <code>normalizetest</code> are supplied for this purpose in the package.
</p>


<h3>See Also</h3>

<p><code>SDAAP</code>, <code>SDAP</code> and <code>SDAD</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">    set.seed(123)
    # Prepare training and test set
    train &lt;- c(1:40,51:90,101:140)
    Xtrain &lt;- iris[train,1:4]
    nX &lt;- normalize(Xtrain)
    Xtrain &lt;- nX$Xc
    Ytrain &lt;- iris[train,5]
    Xtest &lt;- iris[-train,1:4]
    Xtest &lt;- normalizetest(Xtest,nX)
    Ytest &lt;- iris[-train,5]

    # Define parameters for Alternating Direction Method of Multipliers (SDAD)
    Om &lt;- diag(4)+0.1*matrix(1,4,4) #elNet coef mat
    gam &lt;- 0.0001
    lam &lt;- 0.0001
    method &lt;- "SDAD"
    q &lt;- 2
    control &lt;- list(PGsteps = 100,
                    PGtol = c(1e-5,1e-5),
                    mu = 1,
                    maxits = 100,
                    tol = 1e-3,
                    quiet = FALSE)

    # Run the algorithm
    res &lt;- ASDA(Xt = Xtrain,
                Yt = Ytrain,
                Om = Om,
                gam = gam ,
                lam = lam,
                q = q,
                method = method,
                control = control)

    # Can also just use the defaults, which is Accelerated Proximal Gradient (SDAAP):
    resDef &lt;- ASDA(Xtrain,Ytrain)

    # Some example on simulated data
    # Generate Gaussian data on three classes with plenty of redundant variables

    # This example shows the basic steps on how to apply this to data, i.e.:
    #  1) Setup training data
    #  2) Normalize
    #  3) Train
    #  4) Predict
    #  5) Plot projected data
    #  6) Accuracy on test set

    P &lt;- 300 # Number of variables
    N &lt;- 50 # Number of samples per class

    # Mean for classes, they are zero everywhere except the first 3 coordinates
    m1 &lt;- rep(0,P)
    m1[1] &lt;- 3

    m2 &lt;- rep(0,P)
    m2[2] &lt;- 3

    m3 &lt;- rep(0,P)
    m3[3] &lt;- 3

    # Sample dummy data
    Xtrain &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))

    Xtest &lt;- rbind(MASS::mvrnorm(n=N,mu = m1, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m2, Sigma = diag(P)),
                   MASS::mvrnorm(n=N,mu = m3, Sigma = diag(P)))

    # Generate the labels
    Ytrain &lt;- factor(rep(1:3,each=N))
    Ytest &lt;- Ytrain

    # Normalize the data
    Xt &lt;- accSDA::normalize(Xtrain)
    Xtrain &lt;- Xt$Xc # Use the centered and scaled data
    Xtest &lt;- accSDA::normalizetest(Xtest,Xt)

    # Train the classifier and increase the sparsity parameter from the default
    # so we penalize more for non-sparse solutions.
    res &lt;- accSDA::ASDA(Xtrain,Ytrain,lam=0.01)

    # Plot the projected training data, it is projected to
    # 2-dimension because we have 3 classes. The number of discriminant
    # vectors is maximum number of classes minus 1.
    XtrainProjected &lt;- Xtrain%*%res$beta

    plot(XtrainProjected[,1],XtrainProjected[,2],col=Ytrain)

    # Predict on the test data
    preds &lt;- predict(res, newdata = Xtest)

    # Plot projected test data with predicted and correct labels
    XtestProjected &lt;- Xtest%*%res$beta

    plot(XtestProjected[,1],XtestProjected[,2],col=Ytest,
         main="Projected test data with original labels")
    plot(XtestProjected[,1],XtestProjected[,2],col=preds$class,
         main="Projected test data with predicted labels")

    # Calculate accuracy
    sum(preds$class == Ytest)/(3*N) # We have N samples per class, so total 3*N

</code></pre>


</div>