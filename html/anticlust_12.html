<div class="container">

<table style="width: 100%;"><tr>
<td>kplus_moment_variables</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Compute k-plus variables</h2>

<h3>Description</h3>

<p>Compute k-plus variables
</p>


<h3>Usage</h3>

<pre><code class="language-R">kplus_moment_variables(x, T, standardize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A vector, matrix or data.frame of data points. Rows
correspond to elements and columns correspond to features. A
vector represents a single feature.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>T</code></td>
<td>
<p>The number of distribution moments for which variables are generated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>standardize</code></td>
<td>
<p>Logical, should all columns of the output be standardized
(defaults to TRUE).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The k-plus criterion is an extension of the k-means criterion
(i.e., the "variance", see <code>variance_objective</code>).
In <code>kplus_anticlustering</code>, equalizing means and variances
simultaneously (and possibly additional distribution moments) is
accomplished by internally appending new variables to the data
input <code>x</code>. When using only the  variance as additional criterion, the
new variables represent the squared difference of each data point to
the mean of the respective column. All columns are then included—in
addition to the original data—in standard k-means
anticlustering. The logic is readily extended towards higher order moments,
see Papenberg (2024). This function gives users the possibility to generate
k-plus variables themselves, which offers some additional flexibility when
conducting k-plus anticlustering.
</p>


<h3>Value</h3>

<p>A matrix containing all columns of <code>x</code> and all additional
columns of k-plus variables. If <code>x</code> has M columns, the output matrix
has M * T columns.
</p>


<h3>Author(s)</h3>

<p>Martin Papenberg <a href="mailto:martin.papenberg@hhu.de">martin.papenberg@hhu.de</a>
</p>


<h3>References</h3>

<p>Papenberg, M. (2024). K-plus Anticlustering: An Improved k-means Criterion for 
Maximizing Between-Group Similarity. British Journal of Mathematical and 
Statistical Psychology, 77(1), 80–102. https://doi.org/10.1111/bmsp.12315
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Use Schaper data set for example
data(schaper2019)
features &lt;- schaper2019[, 3:6]
K &lt;- 3
N &lt;- nrow(features)

# Some equivalent ways of doing k-plus anticlustering:

init_groups &lt;- sample(rep_len(1:3, N))
table(init_groups)

kplus_groups1 &lt;- anticlustering(
  features,
  K = init_groups,
  objective = "kplus",
  standardize = TRUE,
  method = "local-maximum"
)

kplus_groups2 &lt;- anticlustering(
  kplus_moment_variables(features, T = 2), # standardization included by default
  K = init_groups,
  objective = "variance", # (!)
  method = "local-maximum"
)

# this function uses standardization by default unlike anticlustering():
kplus_groups3 &lt;- kplus_anticlustering(
  features, 
  K = init_groups,
  method = "local-maximum"
)

all(kplus_groups1 == kplus_groups2)
all(kplus_groups1 == kplus_groups3)
all(kplus_groups2 == kplus_groups3)

</code></pre>


</div>