<div class="container">

<table style="width: 100%;"><tr>
<td>Testfunctions</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Optimization Test Functions
</h2>

<h3>Description</h3>

<p>Simple and often used test function defined in higher dimensions and with
analytical gradients, especially suited for performance tests. Analytical 
gradients, where existing, are provided with the <code>gr</code> prefix.
The dimension is determined by the length of the input vector.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fnRosenbrock(x)
grRosenbrock(x)
fnRastrigin(x)
grRastrigin(x)
fnNesterov(x)
grNesterov(x)
fnNesterov1(x)
fnNesterov2(x)
fnHald(x)
grHald(x)
fnShor(x)
grShor(x)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>numeric vector of a certain length.</p>
</td>
</tr></table>
<h3>Details</h3>

<p><b>Rosenbrock</b> – Rosenbrock's famous valley function from 1960. It can
also be regarded as a least-squares problem:
</p>
<p style="text-align: center;"><code class="reqn">\sum_{i=1}^{n-1} (1-x_i)^2 + 100 (x_{i+1}-x_i^2)^2</code>
</p>


<table>
<tr>
<td style="text-align: left;">
  No. of Vars.: </td>
<td style="text-align: left;"> n &gt;= 2 </td>
</tr>
<tr>
<td style="text-align: left;">
  Bounds: </td>
<td style="text-align: left;"> -5.12 &lt;= xi &lt;= 5.12 </td>
</tr>
<tr>
<td style="text-align: left;">
  Local minima: </td>
<td style="text-align: left;"> at f(-1, 1, ..., 1) for n &gt;= 4 </td>
</tr>
<tr>
<td style="text-align: left;">
  Minimum: </td>
<td style="text-align: left;"> 0.0 </td>
</tr>
<tr>
<td style="text-align: left;">
  Solution: </td>
<td style="text-align: left;"> xi = 1, i = 1:n </td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p><b>Nesterov</b> – Nesterov's smooth adaptation of Rosenbrock, based on the 
idea of Chebyshev polynomials. This function is even more difficult to 
optimize than Rosenbrock's:
</p>
<p style="text-align: center;"><code class="reqn">(1 - x_1)^2 / 4 + \sum_{i=1}^{n-1} (1 + x_{i+1} - 2 x_i^2)^2</code>
</p>

<p>Two nonsmooth Nesterov functions are available: Nesterov2 and Nesterov1,
defined as
</p>
<p style="text-align: center;"><code class="reqn">(1 - x_1)^2 / 4 + \sum_{i=1}^{n-1} |1 + x_{i+1} - 2 x_i^2|</code>
</p>

<p style="text-align: center;"><code class="reqn">|1 - x_1| / 4 + \sum_{i=1}^{n-1} (|1 + x_{i+1} - 2 |x_i||</code>
</p>


<table>
<tr>
<td style="text-align: left;">
  No. of Vars.: </td>
<td style="text-align: left;"> n &gt;= 2 </td>
</tr>
<tr>
<td style="text-align: left;">
  Bounds: </td>
<td style="text-align: left;"> -5.12 &lt;= xi &lt;= 5.12 </td>
</tr>
<tr>
<td style="text-align: left;">
  Local minima: ?</td>
</tr>
<tr>
<td style="text-align: left;">
  Minimum: </td>
<td style="text-align: left;"> 0.0 </td>
</tr>
<tr>
<td style="text-align: left;">
  Solution: </td>
<td style="text-align: left;"> xi = 1, i = 1:n </td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p><b>Nesterov1</b> and <b>Nesterov2</b> – Simlar to <code>Nesterov</code>, except the terms added are taken with absolute value, which makes this function nonsmooth and painful for gradient-based optimization routines; no gradient provided.<br> (Nesterov2 uses absolute instead of quadratic terms.)
</p>
<p><b>Rastrigin</b> – Rastrigin's function is a famous, non-convex example from 1989 for global optimization. It is a typical example of a multimodal function with many local minima:
</p>
<p style="text-align: center;"><code class="reqn">10 n + \sum_1^n (x_i^2 - 10 \cos(2 \pi x_i))</code>
</p>


<table>
<tr>
<td style="text-align: left;">
  No. of Vars.: </td>
<td style="text-align: left;"> n &gt;= 2 </td>
</tr>
<tr>
<td style="text-align: left;">
  Bounds: </td>
<td style="text-align: left;"> -5.12 &lt;= xi &lt;= 5.12 </td>
</tr>
<tr>
<td style="text-align: left;">
  Local minima: </td>
<td style="text-align: left;"> many </td>
</tr>
<tr>
<td style="text-align: left;">
  Minimum: </td>
<td style="text-align: left;"> 0.0 </td>
</tr>
<tr>
<td style="text-align: left;">
  Solution: </td>
<td style="text-align: left;"> xi = 0, i = 1:n </td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p><b>Hald</b> – Hald's function is a typical example of a non-smooth test
function, from Hald and Madsen in 1981.
</p>
<p style="text-align: center;"><code class="reqn">\max_{1 \le i \le n} | \frac{x_1 + x_2 t_i}{1 + x_3 t_i + x_4 t_i^2 + x_5 t_i^3} - \exp(t_i)|</code>
</p>

<p>where <code class="reqn">n = 21</code> and <code class="reqn">t_i = -1 + (i - 1)/10</code> for <code class="reqn">1 \le i \le 21</code>.
</p>

<table>
<tr>
<td style="text-align: left;">
  No. of Vars.: </td>
<td style="text-align: left;"> n =5 </td>
</tr>
<tr>
<td style="text-align: left;">
  Bounds: </td>
<td style="text-align: left;"> -1 &lt;= xi &lt;= 1 </td>
</tr>
<tr>
<td style="text-align: left;">
  Local minima: </td>
<td style="text-align: left;"> ? </td>
</tr>
<tr>
<td style="text-align: left;">
  Minimum: </td>
<td style="text-align: left;"> 0.0001223713 </td>
</tr>
<tr>
<td style="text-align: left;">
  Solution: </td>
<td style="text-align: left;"> (0.99987763,  0.25358844, -0.74660757,  0.24520150, -0.03749029) </td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<p><b>Shor</b> – Shor's function is another typical example of a non-smooth test
function, a benchmark for Shor's R-algorithm.
</p>


<h3>Value</h3>

<p>Returns the values of the test function resp. its gradient at that point.
If an analytical gradient is not available, a function computing the gradient 
numerically will be provided.
</p>


<h3>References</h3>

<p>Search the Internet.
</p>


<h3>Examples</h3>

<pre><code class="language-R">x &lt;- runif(5)
fnHald(x); grHald(x)

# Compare analytical and numerical gradient
shor_gr &lt;- function(x) adagio:::ns.grad(fnShor, x)    # internal gradient
grShor(x); shor_gr(x) 
</code></pre>


</div>