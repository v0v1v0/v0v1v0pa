<div class="container">

<table style="width: 100%;"><tr>
<td>TextEmbeddingClassifierNeuralNet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Text embedding classifier with a neural net</h2>

<h3>Description</h3>

<p>Abstract class for neural nets with 'keras'/'tensorflow' and
'pytorch'.
</p>


<h3>Value</h3>

<p>Objects of this class are used for assigning texts to classes/categories. For
the creation and training of a classifier an object of class EmbeddedText and a <code>factor</code>
are necessary. The object of class EmbeddedText contains the numerical text
representations (text embeddings) of the raw texts generated by an object of class
TextEmbeddingModel. The <code>factor</code> contains the classes/categories for every
text. Missing values (unlabeled cases) are supported. For predictions an object of class
EmbeddedText has to be used which was created with the same text embedding model as
for training.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>model</code></dt>
<dd>
<p>('tensorflow_model()')<br>
Field for storing the tensorflow model after loading.</p>
</dd>
<dt><code>model_config</code></dt>
<dd>
<p>('list()')<br>
List for storing information about the configuration of the model. This
information is used to predict new data.
</p>

<ul>
<li>
<p><code>model_config$n_rec: </code>Number of recurrent layers.
</p>
</li>
<li>
<p><code>model_config$n_hidden: </code>Number of dense layers.
</p>
</li>
<li>
<p><code>model_config$target_levels: </code>Levels of the target variable. Do not change this manually.
</p>
</li>
<li>
<p><code>model_config$input_variables: </code>Order and name of the input variables. Do not change this manually.
</p>
</li>
<li>
<p><code>model_config$init_config: </code>List storing all parameters passed to method new().
</p>
</li>
</ul>
</dd>
<dt><code>last_training</code></dt>
<dd>
<p>('list()')<br>
List for storing the history and the results of the last training. This
information will be overwritten if a new training is started.
</p>

<ul>
<li>
<p><code>last_training$learning_time: </code>Duration of the training process.
</p>
</li>
<li>
<p><code>config$history: </code>History of the last training.
</p>
</li>
<li>
<p><code>config$data: </code>Object of class table storing the initial frequencies of the passed data.
</p>
</li>
<li>
<p><code>config$data_pb:l </code>Matrix storing the number of additional cases (test and training) added
during balanced pseudo-labeling. The rows refer to folds and final training.
The columns refer to the steps during pseudo-labeling.
</p>
</li>
<li>
<p><code>config$data_bsc_test: </code>Matrix storing the number of cases for each category used for testing
during the phase of balanced synthetic units. Please note that the
frequencies include original and synthetic cases. In case the number
of original and synthetic cases exceeds the limit for the majority classes,
the frequency represents the number of cases created by cluster analysis.
</p>
</li>
<li>
<p><code>config$date: </code>Time when the last training finished.
</p>
</li>
<li>
<p><code>config$config: </code>List storing which kind of estimation was requested during the last training.
</p>

<ul>
<li>
<p><code>config$config$use_bsc:  </code><code>TRUE</code> if  balanced synthetic cases were requested. <code>FALSE</code>
if not.
</p>
</li>
<li>
<p><code>config$config$use_baseline: </code><code>TRUE</code> if baseline estimation were requested. <code>FALSE</code>
if not.
</p>
</li>
<li>
<p><code> config$config$use_bpl: </code><code>TRUE</code> if  balanced, pseudo-labeling cases were requested. <code>FALSE</code>
if not.
</p>
</li>
</ul>
</li>
</ul>
</dd>
<dt><code>reliability</code></dt>
<dd>
<p>('list()')<br>
List for storing central reliability measures of the last training.
</p>

<ul>
<li>
<p><code>reliability$test_metric: </code>Array containing the reliability measures for the validation data for
every fold, method, and step (in case of pseudo-labeling).
</p>
</li>
<li>
<p><code>reliability$test_metric_mean: </code>Array containing the reliability measures for the validation data for
every method and step (in case of pseudo-labeling). The values represent
the mean values for every fold.
</p>
</li>
<li>
<p><code>reliability$raw_iota_objects: </code>List containing all iota_object generated with the package <code>iotarelr</code>
for every fold at the start and the end of the last training.
</p>

<ul>
<li>
<p><code>reliability$raw_iota_objects$iota_objects_start: </code>List of objects with class <code>iotarelr_iota2</code> containing the
estimated iota reliability of the second generation for the baseline model
for every fold.
If the estimation of the baseline model is not requested, the list is
set to <code>NULL</code>.
</p>
</li>
<li>
<p><code>reliability$raw_iota_objects$iota_objects_end: </code>List of objects with class <code>iotarelr_iota2</code> containing the
estimated iota reliability of the second generation for the final model
for every fold. Depending of the requested training method these values
refer to the baseline model, a trained model on the basis of balanced
synthetic cases, balanced pseudo labeling or a combination of balanced
synthetic cases with pseudo labeling.
</p>
</li>
<li>
<p><code>reliability$raw_iota_objects$iota_objects_start_free: </code>List of objects with class <code>iotarelr_iota2</code> containing the
estimated iota reliability of the second generation for the baseline model
for every fold.
If the estimation of the baseline model is not requested, the list is
set to <code>NULL</code>.Please note that the model is estimated without
forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.
</p>
</li>
<li>
<p><code>reliability$raw_iota_objects$iota_objects_end_free: </code>List of objects with class <code>iotarelr_iota2</code> containing the
estimated iota reliability of the second generation for the final model
for every fold. Depending of the requested training method, these values
refer to the baseline model, a trained model on the basis of balanced
synthetic cases, balanced pseudo-labeling or a combination of balanced
synthetic cases and pseudo-labeling.
Please note that the model is estimated without
forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.
</p>
</li>
</ul>
</li>
<li>
<p><code>reliability$iota_object_start: </code>Object of class <code>iotarelr_iota2</code> as a mean of the individual objects
for every fold. If the estimation of the baseline model is not requested, the list is
set to <code>NULL</code>.
</p>
</li>
<li>
<p><code> reliability$iota_object_start_free: </code>Object of class <code>iotarelr_iota2</code> as a mean of the individual objects
for every fold. If the estimation of the baseline model is not requested, the list is
set to <code>NULL</code>.
Please note that the model is estimated without
forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.
</p>
</li>
<li>
<p><code>reliability$iota_object_end: </code>Object of class <code>iotarelr_iota2</code> as a mean of the individual objects
for every fold.
Depending on the requested training method, this object
refers to the baseline model, a trained model on the basis of balanced
synthetic cases, balanced pseudo-labeling or a combination of balanced
synthetic cases and pseudo-labeling.
</p>
</li>
<li>
<p><code>reliability$iota_object_end_free: </code>Object of class <code>iotarelr_iota2</code> as a mean of the individual objects
for every fold.
Depending on the requested training method, this object
refers to the baseline model, a trained model on the basis of balanced
synthetic cases, balanced pseudo-labeling or a combination of balanced
synthetic cases and pseudo-labeling.
Please note that the model is estimated without
forcing the Assignment Error Matrix to be in line with the assumption of weak superiority.
</p>
</li>
<li>
<p><code>reliability$standard_measures_end: </code>Object of class <code>list</code> containing the final
measures for precision, recall, and f1 for every fold.
Depending of the requested training method, these values
refer to the baseline model, a trained model on the basis of balanced
synthetic cases, balanced pseudo-labeling or a combination of balanced
synthetic cases and pseudo-labeling.
</p>
</li>
<li>
<p><code>reliability$standard_measures_mean: </code><code>matrix</code> containing the mean
measures for precision, recall, and f1 at the end of every fold.
</p>
</li>
</ul>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-new"><code>TextEmbeddingClassifierNeuralNet$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-train"><code>TextEmbeddingClassifierNeuralNet$train()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-predict"><code>TextEmbeddingClassifierNeuralNet$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-check_embedding_model"><code>TextEmbeddingClassifierNeuralNet$check_embedding_model()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_model_info"><code>TextEmbeddingClassifierNeuralNet$get_model_info()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_text_embedding_model"><code>TextEmbeddingClassifierNeuralNet$get_text_embedding_model()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-set_publication_info"><code>TextEmbeddingClassifierNeuralNet$set_publication_info()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_publication_info"><code>TextEmbeddingClassifierNeuralNet$get_publication_info()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-set_software_license"><code>TextEmbeddingClassifierNeuralNet$set_software_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_software_license"><code>TextEmbeddingClassifierNeuralNet$get_software_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-set_documentation_license"><code>TextEmbeddingClassifierNeuralNet$set_documentation_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_documentation_license"><code>TextEmbeddingClassifierNeuralNet$get_documentation_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-set_model_description"><code>TextEmbeddingClassifierNeuralNet$set_model_description()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_model_description"><code>TextEmbeddingClassifierNeuralNet$get_model_description()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-save_model"><code>TextEmbeddingClassifierNeuralNet$save_model()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-load_model"><code>TextEmbeddingClassifierNeuralNet$load_model()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_package_versions"><code>TextEmbeddingClassifierNeuralNet$get_package_versions()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_sustainability_data"><code>TextEmbeddingClassifierNeuralNet$get_sustainability_data()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-get_ml_framework"><code>TextEmbeddingClassifierNeuralNet$get_ml_framework()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingClassifierNeuralNet-clone"><code>TextEmbeddingClassifierNeuralNet$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creating a new instance of this class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$new(
  ml_framework = aifeducation_config$get_framework(),
  name = NULL,
  label = NULL,
  text_embeddings = NULL,
  targets = NULL,
  hidden = c(128),
  rec = c(128),
  self_attention_heads = 0,
  intermediate_size = NULL,
  attention_type = "fourier",
  add_pos_embedding = TRUE,
  rec_dropout = 0.1,
  repeat_encoder = 1,
  dense_dropout = 0.4,
  recurrent_dropout = 0.4,
  encoder_dropout = 0.1,
  optimizer = "adam"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt>
<dd>
<p><code>string</code> Framework to use for training and inference.
<code>ml_framework="tensorflow"</code> for 'tensorflow' and <code>ml_framework="pytorch"</code>
for 'pytorch'</p>
</dd>
<dt><code>name</code></dt>
<dd>
<p><code>Character</code> Name of the new classifier. Please refer to
common name conventions. Free text can be used with parameter <code>label</code>.</p>
</dd>
<dt><code>label</code></dt>
<dd>
<p><code>Character</code> Label for the new classifier. Here you can use
free text.</p>
</dd>
<dt><code>text_embeddings</code></dt>
<dd>
<p>An object of class<code>TextEmbeddingModel</code>.</p>
</dd>
<dt><code>targets</code></dt>
<dd>
<p><code>factor</code> containing the target values of the classifier.</p>
</dd>
<dt><code>hidden</code></dt>
<dd>
<p><code>vector</code> containing the number of neurons for each dense layer.
The length of the vector determines the number of dense layers. If you want no dense layer,
set this parameter to <code>NULL</code>.</p>
</dd>
<dt><code>rec</code></dt>
<dd>
<p><code>vector</code> containing the number of neurons for each recurrent layer.
The length of the vector determines the number of dense layers. If you want no dense layer,
set this parameter to <code>NULL</code>.</p>
</dd>
<dt><code>self_attention_heads</code></dt>
<dd>
<p><code>integer</code> determining the number of attention heads
for a self-attention layer. Only relevant if <code>attention_type="multihead"</code></p>
</dd>
<dt><code>intermediate_size</code></dt>
<dd>
<p><code>int</code> determining the size of the projection layer within
a each transformer encoder.</p>
</dd>
<dt><code>attention_type</code></dt>
<dd>
<p><code>string</code> Choose the relevant attention type. Possible values
are <code>"fourier"</code> and <code>multihead</code>.</p>
</dd>
<dt><code>add_pos_embedding</code></dt>
<dd>
<p><code>bool</code> <code>TRUE</code> if positional embedding should be used.</p>
</dd>
<dt><code>rec_dropout</code></dt>
<dd>
<p><code>double</code> ranging between 0 and lower 1, determining the
dropout between bidirectional gru layers.</p>
</dd>
<dt><code>repeat_encoder</code></dt>
<dd>
<p><code>int</code> determining how many times the encoder should be
added to the network.</p>
</dd>
<dt><code>dense_dropout</code></dt>
<dd>
<p><code>double</code> ranging between 0 and lower 1, determining the
dropout between dense layers.</p>
</dd>
<dt><code>recurrent_dropout</code></dt>
<dd>
<p><code>double</code> ranging between 0 and lower 1, determining the
recurrent dropout for each recurrent layer. Only relevant for keras models.</p>
</dd>
<dt><code>encoder_dropout</code></dt>
<dd>
<p><code>double</code> ranging between 0 and lower 1, determining the
dropout for the dense projection within the encoder layers.</p>
</dd>
<dt><code>optimizer</code></dt>
<dd>
<p>Object of class <code>keras.optimizers</code>.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Returns an object of class TextEmbeddingClassifierNeuralNet which is ready for
training.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-train"></a>



<h4>Method <code>train()</code>
</h4>

<p>Method for training a neural net.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$train(
  data_embeddings,
  data_targets,
  data_n_test_samples = 5,
  balance_class_weights = TRUE,
  use_baseline = TRUE,
  bsl_val_size = 0.25,
  use_bsc = TRUE,
  bsc_methods = c("dbsmote"),
  bsc_max_k = 10,
  bsc_val_size = 0.25,
  bsc_add_all = FALSE,
  use_bpl = TRUE,
  bpl_max_steps = 3,
  bpl_epochs_per_step = 1,
  bpl_dynamic_inc = FALSE,
  bpl_balance = FALSE,
  bpl_max = 1,
  bpl_anchor = 1,
  bpl_min = 0,
  bpl_weight_inc = 0.02,
  bpl_weight_start = 0,
  bpl_model_reset = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  epochs = 40,
  batch_size = 32,
  dir_checkpoint,
  trace = TRUE,
  keras_trace = 2,
  pytorch_trace = 2,
  n_cores = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_embeddings</code></dt>
<dd>
<p>Object of class <code>TextEmbeddingModel</code>.</p>
</dd>
<dt><code>data_targets</code></dt>
<dd>
<p><code>Factor</code> containing the labels for cases
stored in <code>data_embeddings</code>. Factor must be named and has to use the
same names used in <code>data_embeddings</code>.</p>
</dd>
<dt><code>data_n_test_samples</code></dt>
<dd>
<p><code>int</code> determining the number of cross-fold
samples.</p>
</dd>
<dt><code>balance_class_weights</code></dt>
<dd>
<p><code>bool</code> If <code>TRUE</code> class weights are
generated based on the frequencies of the training data with the method
Inverse Class Frequency'. If <code>FALSE</code> each class has the weight 1.</p>
</dd>
<dt><code>use_baseline</code></dt>
<dd>
<p><code>bool</code> <code>TRUE</code> if the calculation of a baseline
model is requested. This option is only relevant for <code>use_bsc=TRUE</code> or
<code>use_pbl=TRUE</code>. If both are <code>FALSE</code>, a baseline model is calculated.</p>
</dd>
<dt><code>bsl_val_size</code></dt>
<dd>
<p><code>double</code> between 0 and 1, indicating the proportion of cases of each class
which should be used for the validation sample during the estimation of the baseline model.
The remaining cases are part of the training data.</p>
</dd>
<dt><code>use_bsc</code></dt>
<dd>
<p><code>bool</code> <code>TRUE</code> if the estimation should integrate
balanced synthetic cases. <code>FALSE</code> if not.</p>
</dd>
<dt><code>bsc_methods</code></dt>
<dd>
<p><code>vector</code> containing the methods for generating
synthetic cases via 'smotefamily'. Multiple methods can
be passed. Currently <code>bsc_methods=c("adas")</code>, <code>bsc_methods=c("smote")</code>
and <code>bsc_methods=c("dbsmote")</code> are possible.</p>
</dd>
<dt><code>bsc_max_k</code></dt>
<dd>
<p><code>int</code> determining the maximal number of k which is used
for creating synthetic units.</p>
</dd>
<dt><code>bsc_val_size</code></dt>
<dd>
<p><code>double</code> between 0 and 1, indicating the proportion of cases of each class
which should be used for the validation sample during the estimation with synthetic cases.</p>
</dd>
<dt><code>bsc_add_all</code></dt>
<dd>
<p><code>bool</code> If <code>FALSE</code> only synthetic cases necessary to fill
the gab between the class and the major class are added to the data. If <code>TRUE</code> all
generated synthetic cases are added to the data.</p>
</dd>
<dt><code>use_bpl</code></dt>
<dd>
<p><code>bool</code> <code>TRUE</code> if the estimation should integrate
balanced pseudo-labeling. <code>FALSE</code> if not.</p>
</dd>
<dt><code>bpl_max_steps</code></dt>
<dd>
<p><code>int</code> determining the maximum number of steps during
pseudo-labeling.</p>
</dd>
<dt><code>bpl_epochs_per_step</code></dt>
<dd>
<p><code>int</code> Number of training epochs within every step.</p>
</dd>
<dt><code>bpl_dynamic_inc</code></dt>
<dd>
<p><code>bool</code> If <code>TRUE</code>, only a specific percentage
of cases is included during each step. The percentage is determined by
<code class="reqn">step/bpl_max_steps</code>. If <code>FALSE</code>, all cases are used.</p>
</dd>
<dt><code>bpl_balance</code></dt>
<dd>
<p><code>bool</code> If <code>TRUE</code>, the same number of cases for
every category/class of the pseudo-labeled data are used with training. That
is, the number of cases is determined by the minor class/category.</p>
</dd>
<dt><code>bpl_max</code></dt>
<dd>
<p><code>double</code> between 0 and 1, setting the maximal level of
confidence for considering a case for pseudo-labeling.</p>
</dd>
<dt><code>bpl_anchor</code></dt>
<dd>
<p><code>double</code> between 0 and 1 indicating the reference
point for sorting the new cases of every label. See notes for more details.</p>
</dd>
<dt><code>bpl_min</code></dt>
<dd>
<p><code>double</code> between 0 and 1, setting the minimal level of
confidence for considering a case for pseudo-labeling.</p>
</dd>
<dt><code>bpl_weight_inc</code></dt>
<dd>
<p><code>double</code> value how much the sample weights
should be increased for the cases with pseudo-labels in every step.</p>
</dd>
<dt><code>bpl_weight_start</code></dt>
<dd>
<p><code>dobule</code> Starting value for the weights of the
unlabeled cases.</p>
</dd>
<dt><code>bpl_model_reset</code></dt>
<dd>
<p><code>bool</code> If <code>TRUE</code>, model is re-initialized at every
step.</p>
</dd>
<dt><code>sustain_track</code></dt>
<dd>
<p><code>bool</code> If <code>TRUE</code> energy consumption is tracked
during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt>
<dd>
<p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable
must be set if sustainability should be tracked. A list can be found on
Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt>
<dd>
<p>Region within a country. Only available for USA and
Canada See the documentation of codecarbon for more information.
<a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a></p>
</dd>
<dt><code>sustain_interval</code></dt>
<dd>
<p><code>integer</code> Interval in seconds for measuring power
usage.</p>
</dd>
<dt><code>epochs</code></dt>
<dd>
<p><code>int</code> Number of training epochs.</p>
</dd>
<dt><code>batch_size</code></dt>
<dd>
<p><code>int</code> Size of batches.</p>
</dd>
<dt><code>dir_checkpoint</code></dt>
<dd>
<p><code>string</code> Path to the directory where
the checkpoint during training should be saved. If the directory does not
exist, it is created.</p>
</dd>
<dt><code>trace</code></dt>
<dd>
<p><code>bool</code> <code>TRUE</code>, if information about the estimation
phase should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt>
<dd>
<p><code>int</code> <code>keras_trace=0</code> does not print any
information about the training process from keras on the console.</p>
</dd>
<dt><code>pytorch_trace</code></dt>
<dd>
<p><code>int</code> <code>pytorch_trace=0</code> does not print any
information about the training process from pytorch on the console.
<code>pytorch_trace=1</code> prints a progress bar. <code>pytorch_trace=2</code> prints
one line of information for every epoch.</p>
</dd>
<dt><code>n_cores</code></dt>
<dd>
<p><code>int</code> Number of cores used for creating synthetic units.</p>
</dd>
</dl>
</div>



<h5>Details</h5>


<ul>
<li>
<p><code>bsc_max_k: </code>All values from 2 up to bsc_max_k are successively used. If
the number of bsc_max_k is too high, the value is reduced to a number that
allows the calculating of synthetic units.
</p>
</li>
<li>
<p><code>bpl_anchor: </code>With the help of this value, the new cases are sorted. For
this aim, the distance from the anchor is calculated and all cases are arranged
into an ascending order.

</p>
</li>
</ul>
<h5>Returns</h5>

<p>Function does not return a value. It changes the object into a trained
classifier.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-predict"></a>



<h4>Method <code>predict()</code>
</h4>

<p>Method for predicting new data with a trained neural net.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$predict(newdata, batch_size = 32, verbose = 1)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>newdata</code></dt>
<dd>
<p>Object of class <code>TextEmbeddingModel</code> or
<code>data.frame</code> for which predictions should be made.</p>
</dd>
<dt><code>batch_size</code></dt>
<dd>
<p><code>int</code> Size of batches.</p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p><code>int</code> <code>verbose=0</code> does not cat any
information about the training process from keras on the console.
<code>verbose=1</code> prints a progress bar. <code>verbose=2</code> prints
one line of information for every epoch.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Returns a <code>data.frame</code> containing the predictions and
the probabilities of the different labels for each case.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-check_embedding_model"></a>



<h4>Method <code>check_embedding_model()</code>
</h4>

<p>Method for checking if the provided text embeddings are
created with the same TextEmbeddingModel as the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$check_embedding_model(text_embeddings)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>text_embeddings</code></dt>
<dd>
<p>Object of class EmbeddedText.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>TRUE</code> if the underlying TextEmbeddingModel are the same.
<code>FALSE</code> if the models differ.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_model_info"></a>



<h4>Method <code>get_model_info()</code>
</h4>

<p>Method for requesting the model information
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_model_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> of all relevant model information
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_text_embedding_model"></a>



<h4>Method <code>get_text_embedding_model()</code>
</h4>

<p>Method for requesting the text embedding model information
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_text_embedding_model()</pre></div>



<h5>Returns</h5>

<p><code>list</code> of all relevant model information on the text embedding model
underlying the classifier
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-set_publication_info"></a>



<h4>Method <code>set_publication_info()</code>
</h4>

<p>Method for setting publication information of the classifier
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$set_publication_info(
  authors,
  citation,
  url = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>authors</code></dt>
<dd>
<p>List of authors.</p>
</dd>
<dt><code>citation</code></dt>
<dd>
<p>Free text citation.</p>
</dd>
<dt><code>url</code></dt>
<dd>
<p>URL of a corresponding homepage.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private
members for publication information.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_publication_info"></a>



<h4>Method <code>get_publication_info()</code>
</h4>

<p>Method for requesting the bibliographic information of the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_publication_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> with all saved bibliographic information.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-set_software_license"></a>



<h4>Method <code>set_software_license()</code>
</h4>

<p>Method for setting the license of the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$set_software_license(license = "GPL-3")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt>
<dd>
<p><code>string</code> containing the abbreviation of the license or
the license text.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private member for
the software license of the model.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_software_license"></a>



<h4>Method <code>get_software_license()</code>
</h4>

<p>Method for getting the license of the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_software_license()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt>
<dd>
<p><code>string</code> containing the abbreviation of the license or
the license text.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>string</code> representing the license for the software.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-set_documentation_license"></a>



<h4>Method <code>set_documentation_license()</code>
</h4>

<p>Method for setting the license of the classifier's documentation.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$set_documentation_license(
  license = "CC BY-SA"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt>
<dd>
<p><code>string</code> containing the abbreviation of the license or
the license text.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private member for
the documentation license of the model.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_documentation_license"></a>



<h4>Method <code>get_documentation_license()</code>
</h4>

<p>Method for getting the license of the classifier's documentation.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_documentation_license()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt>
<dd>
<p><code>string</code> containing the abbreviation of the license or
the license text.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Returns the license as a <code>string</code>.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-set_model_description"></a>



<h4>Method <code>set_model_description()</code>
</h4>

<p>Method for setting a description of the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$set_model_description(
  eng = NULL,
  native = NULL,
  abstract_eng = NULL,
  abstract_native = NULL,
  keywords_eng = NULL,
  keywords_native = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>eng</code></dt>
<dd>
<p><code>string</code> A text describing the training of the learner,
its theoretical and empirical background, and the different output labels
in English.</p>
</dd>
<dt><code>native</code></dt>
<dd>
<p><code>string</code> A text describing the training of the learner,
its theoretical and empirical background, and the different output labels
in the native language of the classifier.</p>
</dd>
<dt><code>abstract_eng</code></dt>
<dd>
<p><code>string</code> A text providing a summary of the description
in English.</p>
</dd>
<dt><code>abstract_native</code></dt>
<dd>
<p><code>string</code> A text providing a summary of the description
in the native language of the classifier.</p>
</dd>
<dt><code>keywords_eng</code></dt>
<dd>
<p><code>vector</code> of keyword in English.</p>
</dd>
<dt><code>keywords_native</code></dt>
<dd>
<p><code>vector</code> of keyword in the native language of the classifier.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private members for the
description of the model.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_model_description"></a>



<h4>Method <code>get_model_description()</code>
</h4>

<p>Method for requesting the model description.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_model_description()</pre></div>



<h5>Returns</h5>

<p><code>list</code> with the description of the classifier in English
and the native language.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-save_model"></a>



<h4>Method <code>save_model()</code>
</h4>

<p>Method for saving a model to 'Keras v3 format',
'tensorflow' SavedModel format or h5 format.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$save_model(dir_path, save_format = "default")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt>
<dd>
<p><code>string()</code> Path of the directory where the model should be
saved.</p>
</dd>
<dt><code>save_format</code></dt>
<dd>
<p>Format for saving the model. For 'tensorflow'/'keras' models
<code>"keras"</code> for 'Keras v3 format',
<code>"tf"</code> for SavedModel
or <code>"h5"</code> for HDF5.
For 'pytorch' models <code>"safetensors"</code> for 'safetensors' or
<code>"pt"</code> for 'pytorch' via pickle.
Use <code>"default"</code> for the standard format. This is keras for
'tensorflow'/'keras' models and safetensors for 'pytorch' models.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Function does not return a value. It saves the model to disk.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-load_model"></a>



<h4>Method <code>load_model()</code>
</h4>

<p>Method for importing a model from 'Keras v3 format',
'tensorflow' SavedModel format or h5 format.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$load_model(dir_path, ml_framework = "auto")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt>
<dd>
<p><code>string()</code> Path of the directory where the model is
saved.</p>
</dd>
<dt><code>ml_framework</code></dt>
<dd>
<p><code>string</code> Determines the machine learning framework
for using the model. Possible are <code>ml_framework="pytorch"</code> for 'pytorch',
<code>ml_framework="tensorflow"</code> for 'tensorflow', and <code>ml_framework="auto"</code>.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used to load the weights
of a model.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_package_versions"></a>



<h4>Method <code>get_package_versions()</code>
</h4>

<p>Method for requesting a summary of the R and python packages'
versions used for creating the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_package_versions()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> containing the versions of the relevant
R and python packages.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_sustainability_data"></a>



<h4>Method <code>get_sustainability_data()</code>
</h4>

<p>Method for requesting a summary of tracked energy consumption
during training and an estimate of the resulting CO2 equivalents in kg.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_sustainability_data()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> containing the tracked energy consumption,
CO2 equivalents in kg, information on the tracker used, and technical
information on the training infrastructure.
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-get_ml_framework"></a>



<h4>Method <code>get_ml_framework()</code>
</h4>

<p>Method for requesting the machine learning framework used
for the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$get_ml_framework()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>string</code> describing the machine learning framework used
for the classifier
</p>


<hr>
<a id="method-TextEmbeddingClassifierNeuralNet-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingClassifierNeuralNet$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




</div>